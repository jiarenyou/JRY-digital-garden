# 机器学习--周志华

## 一   绪论

### 1.2基本术语

**泛化**：机器学习的目标是使模型不仅能够适应样本，而且能够适用于新的样本，学得模型的适用新样本的能力成为“泛化”。

通常假设样本空间中全 体样本服从 个未知"分布" (distribution) 我们获得的每个样本都是独立 地从这个分布上采样获得的，即"**独立同分布**" (independent and identically  distributed ，简称 i.i.d.). 一般而言，训练样本越多，我们得到的关于 的信息越多，这样就越有可能通过学习获得具有强泛化能力的模型.

### 1.3 假设空间

归纳：从样例中学习归纳出一般规律，这个过程成为“归纳学习”

演绎：基于公理和推理规则推导相恰的定理

归纳学习：归纳学习有狭义与广义之分?广义的归纳学习大体相当于从样例中学习， 而狭义的归纳学习则要求从训练数据中学得概念(concept) ，因此亦称为"概念 学习"或"概念形成"概念学习技术目前研究、应用都比较少，因为要学得 泛化性能好且语义明确的概念实在太困难了，现实常用的技术大多是产生"黑 箱"模型.然而，对概念学习有所了解，有助于理解机器学习的一些基础思想.

我们可以把学习过程看作一个在所有假设(hypothesis) 组成的空间中进行 搜索的过程，搜索目标是找到与训练集"匹配"但t) 的假设，即能够将训练集中 的瓜判断正确的假设.假设的表示一旦确定，假设空间及其规模大小就确定了.

### 1.4  归纳偏好

机器学习 算法在学习过程中对某种类型假设的偏好，称为"归纳偏好" (inductive bias) ,  或简称为"偏好"

"没有免费的午餐"定理 (No Free Lunch Theorem，简称 NFL 这里的简化论述繁难得多. 定理) [Wolpert, 1996; Wolpert and Macready, 1995].

NFL 定理最重要的寓意?是让我们清楚地认识到，脱离具体问题，空 泛地谈论"什么学习算法更好"毫无意义，因为若考虑所有潜在的问题，贝。所 有学习算法都一样好.要谈论算法的相对优劣，必须要针对具体的学习问题;在 某些问题上表现好的学习算法，在另一些问题上却可能不尽如人意，学习算法 自身的归纳偏好与问题是否相配，往往会起到决定性的作用.

### 1.5 发展历程

​	大体来说，数据库领域的研究为数据挖掘提供数据管 理技术?而机器学习和统计学的研究为数据挖掘提供数据分析技术.由于统计 学界的研究成果通常需要经由机器学习研究来形成有效的学习算法，之后再进 入数据挖掘领域，因此从这个意义上说，统计学主要是通过机器学习对数据挖 掘发挥影响，而机器学习领域和数据库领域则是数据挖掘的两大支撑.

## 二  模型评估与选择

### 2.1 经验误差与过拟合

$$
错误率 = 1 - 精度
$$

错误率：E= α/m

精度：精度=1-错误率

​	更一般地，我们把 学习器的实际预测输出与样本的真实输出之间的差异称为"**误差**" (error),  学习器在训练集上的误差称为"**训练误差**" (training error) 或"**经验误差**" (empirical error) ，在新样本上的误差称为"**泛化误差**" (generalization  error).

​	过拟合是机器学习面临的关键障碍，各类学习算法都必然带有一 些针对过拟合的措施;然而必须认识到，过拟合是无法彻底避免的，我们所能做 的只是"缓解'气或者说减小其风险

​	我们该选 哪个学习算法、使用哪 种参数配置呢?这就 器学习 中的"模型选择" (model  selection ）问题.理想的解决方案当然是对候选模型的泛化误差进 评估 然后 选择泛化 差最小的那个 型.然而如上面所讨论的，我们无法直接获得泛化误差，而训练误差又由于过拟合现象的存在而不 作为标准，那么，在 实中 如何进行模型评估与选择呢?

### 2.2  评估方法

​	通常，我们可通过实验测试对学习器的泛化误差进行评估并进而做出选择，为此，需要使用一个“测试集”来测试学习器对新鲜样本的判别能力，然后以测试集上的“测试误差”作为泛化误差的近似。通常我们假设测试样本也是从严本真实分布中独立同分布采样而得，但需要注意的是，测试集应该尽可能与训练集互斥，即测试样本尽量不在训练集中出现、未在训练过程中使用过。

#### 2.2.1 留出法

​	"留出法" (hold-out) 直接将数据集 划分为两个互斥的集合?其中一个 集合作为训练集 ，另一个作为测试集 D=BUT T= 正~.在 上训 练出模型后，用 来评估其测试误差，作为对泛化误差的估计.

​	在使用留出法时，一般要采用若干次随机划分、重复进行实验进行评估后取平均值作为留出法的评估结果，例如进行100次随机划分，每次产生一个训练/测试集用于实验评估。

​	使用留出法有个窘境，若训练集S包含绝大多数样本，训练处模型更接近，但测试集T由于样本量少，评估结果可能不够稳定；相反，若T包含更多样本，可能得出的模型与真实模型差别更大，从而降低了评估结果的保真性。**这个问题没有完美的解决方案 常见做法是将大约 rv 4/5 样本用于训练，剩余样本用 测试.**

#### 2.2.2  交叉验证法

![image.png](https://build-web.oss-cn-qingdao.aliyuncs.com/my_pic_file/20250810134618.png)


​	"**交叉验证法**" (cross alidation) 将数据集D划分为 **k**个大小相似的 互斥子集， D = D1 ∪D2∪... U Dk， Di ∩ Dj = ø (í 每个子集 尽可能保持数据分布的一致性，即从D中通过分层采样得到.，然后，每次用 k-1 子集的并集作为训练集，余下的那个子集作为测试集；这样就可获得 k组训练 /试集，从而可进行 k次训练和测试，最终返回的是 k个测试结果 的均值 ，显然，交叉验证法评估结果的稳定 和保真性 在很大程 上取决于 k的取值，为强调这一点，通常把交叉验证法称为 “k折交叉验证" (k-fold cross  validation). k最常用 的取 10 ，此时称为 10折交叉验证。

​	假定数据集D中包含m个样本，若令k=m，则得到交叉验证的一个特例：**留一法**（leave-one-out，简称LOO），留一法的评估结果往往被认为比较准确，**缺陷是不适合较大的数据集**

#### 2.2.3 自助法

Bootstrap本意是"解靴带"这里是在使用德国 18 世纪文学作品《吹牛 大王历险记》中解靴带自 助的典故，因此本书译为 "自助法" 自助采样亦 称"可重复采样"或"有 放回采样"。

自助法在数据集较小、难以有效划分训练/测试集时很有用;此外，自助法 能从初始数据集中产生多个不同的训练集，这对集成学习等方法有很大的好处. 然而，自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差.因此，在初始数据量足够时，留出法和交叉验证法更常用一些.

#### 2.2.4  调参与最终模型

​	大多数学习算法都有些参数(parameter) 需要设定，参数配置不同，学得模 型的性能往往有显著差别.因此，在进行模型评估与选择时，除了要对适用学习 算法进行选择，还需对算法参数进行设定，这就是通常所说的"参数调节"或 简称"调参" (parameter tuning).

​	对每种参数 配置都训练出模型来是不可行的.现实中常用的做法是对每个参数选定一个 范围和变化步长，例如在 [0 0.2] 范围内以 0.05 为步长，则实际要评估的候选参 数值有 5个，最终是从这5 个候选值中产生选定值.显然，这样选定的参数值往 往不是"最佳"值，但这是在计算开销和性能估计之间进行折中的结果，通过 这个折中，学习过程才变得可行

### 2.3 性能度量

对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需 要有衡量模型泛化能力的评价标准，这就是性能度量(performance measure)

性能度量反映了任务需求，在对比不同模型的能力时，使用不同的性能度量往 往会导致不同的评判结果;这意味着模型的"好坏"是相对的，什么样的模型 是好的?不仅取决于算法和数据，还决定于任务需求.

#### 2.3.1 错误率和精度

错误率和精度是分类问题中最常用的两种性能度量，既适用于二分类任务，也适用于多分类任务。

#### 2.3.2 查准率、查全率与F1

查准率亦称"精确率" ，查全率亦称"召回率"

![image.png](https://build-web.oss-cn-qingdao.aliyuncs.com/my_pic_file/20250810134647.png)


准确率和召回率是一对矛盾的度量，一般来说，准确率高，召回率偏低；而召回率高时，准确率低，

一般来说，Precision就是检索出来的条目（比如：文档、网页等）有多少是准确的，Recall就是所有准确的条目有多少被检索出来了

![image.png](https://build-web.oss-cn-qingdao.aliyuncs.com/my_pic_file/20250810134706.png)


其中 β>0度量了查全率对查准率的相对重要性 [Van Rijsbergen, 1979]。β = 1 时退化为标准的 F1; β > 1时查全率有更大影响 β <1 时查准率有更大影响.

### 2.3.3 ROC与AUC

