{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1978fdb8-5b21-45df-925c-1646d2e0325a",
   "metadata": {},
   "source": [
    "# 环境安装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7fe710-aee8-45e5-83cf-308ac10b3ecc",
   "metadata": {},
   "source": [
    "    本项目来源于和鲸社区，使用转载需要标注来源\n",
    "    作者: 和鲸社区\n",
    "    来源: 和鲸社区\n",
    "    使用https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple 清华园镜像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b51eb6e-0f87-49cf-9068-12b16be77bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BuildDataset.py     ModelMerge2.py  combined_processed_data_arabic.json\n",
      "DataProcess.py\t    ModelTest1.py   train.py\n",
      "DomainEvaluator.py  ModelTest2.py   阿拉伯语微调实践.ipynb\n",
      "ModelMerge1.py\t    Welcome.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a30b34-90d7-46c4-bfbe-23aaf5bac55a",
   "metadata": {},
   "source": [
    "# 环境导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d04ddd-b577-4af2-92ec-e557e203ae53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "# 设置 HF_ENDPOINT 环境变量\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "print(os.environ.get('HF_ENDPOINT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30c18ebd-1bb8-4967-acd1-d3e5ef592f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:datasets:PyTorch version 2.7.0 available.\n"
     ]
    }
   ],
   "source": [
    "import torch  # PyTorch深度学习框架\n",
    "from datasets import load_dataset, Dataset  # Hugging Face的数据集加载工具\n",
    "from transformers import (  # Hugging Face的转换器库\n",
    "    AutoModelForCausalLM,  # 自回归语言模型（用于生成文本）\n",
    "    AutoTokenizer,  # 自动分词器\n",
    "    AutoModelForMaskedLM,  # 掩码语言模型\n",
    "    get_linear_schedule_with_warmup,  # 学习率预热调度器\n",
    "    DataCollatorForLanguageModeling  # 用于MLM的数据整理器\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType  # 参数高效微调工具\n",
    "from torch.utils.data import DataLoader, random_split  # 数据加载相关工具\n",
    "from torch.optim import AdamW  # Adam优化器的变体\n",
    "from tqdm import tqdm  # 进度条工具\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import jieba\n",
    "import pythainlp\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "from pythainlp.tokenize import word_tokenize as thai_tokenize\n",
    "from pythainlp import pos_tag as thai_pos_tag\n",
    "from typing import List, Dict, Set\n",
    "import langdetect\n",
    "from langdetect.lang_detect_exception import LangDetectException"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e944b4-3520-4656-a8bd-e163837dbfbe",
   "metadata": {},
   "source": [
    "# 模型评估与优化 （确定评估指标）\n",
    "- 困惑度(Perplexity)\n",
    "    - 评估模型在验证集上的表现\n",
    "    - 监控模型是否过拟合\n",
    "- 领域适应性评估\n",
    "    - 术语覆盖率：# term_coverage是术语覆盖率，计算response中包含的领域术语数量占总术语数量的比例\n",
    "    - 术语密度：# term_density是术语密度，计算response中包含的领域术语数量占总token数量的比例\n",
    "    - 响应质量：# response_quality是回复质量，计算response与prompt的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "864efc53-9fd7-4390-8ffe-afeb70868a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainEvaluator:\n",
    "    def __init__(self, tokenizer, device):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        # 加载sentence transformer用于计算文本相似度\n",
    "        self.sentence_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "        self.sentence_model.to(device)\n",
    "        \n",
    "        # 加载领域术语\n",
    "        with open(\"/gemini/data-1/domain_terms_arabic.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            self.domain_terms = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "    def calculate_domain_perplexity(self, model, eval_dataloader):\n",
    "        \"\"\"计算领域数据的困惑度\"\"\"\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in eval_dataloader:\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                total_loss += outputs.loss.item() * batch[\"input_ids\"].size(0)\n",
    "                total_tokens += batch[\"input_ids\"].ne(self.tokenizer.pad_token_id).sum().item()\n",
    "        \n",
    "        return torch.exp(torch.tensor(total_loss / total_tokens))\n",
    "    \n",
    "    def evaluate_domain_adaptation(self, model, texts, lang=None):\n",
    "        \"\"\"评估生成文本的领域适应性\"\"\"\n",
    "        model.eval()\n",
    "        metrics = {\n",
    "            \"term_coverage\": [],\n",
    "            \"term_density\": [],\n",
    "            \"response_quality\": []\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for text in texts:\n",
    "                # 构建提示\n",
    "                if lang:\n",
    "                    prompt = f\"请用{lang}语言回答以下问题：\\n{text}\"\n",
    "                else:\n",
    "                    prompt = text\n",
    "                \n",
    "                # 生成回复\n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=800,\n",
    "                    num_return_sequences=1,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.95\n",
    "                )\n",
    "                response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                \n",
    "                # 计算评估指标\n",
    "                term_count = sum(1 for term in self.domain_terms if term.lower() in response.lower())\n",
    "                term_coverage = term_count / len(self.domain_terms) if self.domain_terms else 0\n",
    "                metrics[\"term_coverage\"].append(term_coverage)\n",
    "                \n",
    "                term_density = term_count / len(response) if response else 0\n",
    "                metrics[\"term_density\"].append(term_density)\n",
    "                \n",
    "                # 计算回复质量\n",
    "                response_embedding = self.sentence_model.encode([response])\n",
    "                prompt_embedding = self.sentence_model.encode([text])\n",
    "                similarity = cosine_similarity(response_embedding, prompt_embedding)[0][0]\n",
    "                metrics[\"response_quality\"].append(similarity)\n",
    "        \n",
    "        return {\n",
    "            \"avg_term_coverage\": np.mean(metrics[\"term_coverage\"]),\n",
    "            \"avg_term_density\": np.mean(metrics[\"term_density\"]),\n",
    "            \"avg_response_quality\": np.mean(metrics[\"response_quality\"])\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a0056a-8f48-4bd6-92a8-5b8137003a56",
   "metadata": {},
   "source": [
    "# 数据处理\n",
    "数据处理流程\n",
    "1. 数据清洗\n",
    "2. 去除HTML标签和特殊字符\n",
    "3. 规范化文本格式\n",
    "4. 过滤过长或过短的文本\n",
    "5. 数据转换\n",
    "6. 构建输入输出对\n",
    "7. 添加任务相关的指令提示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db7bed2c-34db-43fd-95b0-55630f7ef914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55cb1028-b113-4000-95fd-efa93d9a5bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"清理文本内容\"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    # 正则表达式是一种文本匹配模式,下面详细解释每一步:\n",
    "    \n",
    "    # 1. 处理连续换行符\n",
    "    # re.sub()函数用于替换文本,接受3个参数:\n",
    "    # - 第1个参数 r'\\n+' 表示:\n",
    "    #   \\n 代表换行符\n",
    "    #   + 表示匹配1个或多个连续的换行符\n",
    "    # - 第2个参数 '\\n' 表示用单个换行符替换\n",
    "    # - 第3个参数是要处理的文本\n",
    "    # strip()去除文本首尾的空格\n",
    "    text = re.sub(r'\\n+', '\\n', text.strip())\n",
    "    \n",
    "    # 2. 处理连续空格\n",
    "    # r'\\s+' 表示:\n",
    "    # \\s 代表任意空白字符(空格、制表符等)\n",
    "    # + 表示匹配1个或多个连续的空白字符\n",
    "    # 用单个空格替换所有连续的空白字符\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 3. 移除HTML标签，正则表达式 r'<[^>]+>'\n",
    "    # 1) r'' 表示这是一个原始字符串,不会对反斜杠\\进行转义处理\n",
    "    # 2) < 就是匹配HTML标签的开始符号 <\n",
    "    # 3) [^>] 是一个字符集:\n",
    "    #    - [] 表示匹配其中的任意一个字符\n",
    "    #    - ^ 在[]内表示\"非\",即取反\n",
    "    #    - 所以[^>]表示匹配任何不是>的字符\n",
    "    # 4) + 表示\"一个或多个\",即重复前面的[^>]一次或多次\n",
    "    # 5) > 就是匹配HTML标签的结束符号 >\n",
    "    # \n",
    "    # 举例说明:\n",
    "    # 原文本: \"这是<p>一个段落</p>\"\n",
    "    # - <p> 会被匹配,因为它符合模式:<加上任意非>字符(这里是p)再加上>\n",
    "    # - </p> 也会被匹配,因为它符合模式:<加上任意非>字符(这里是/p)再加上>\n",
    "    # \n",
    "    # re.sub()会把所有匹配到的内容替换为空字符串'',所以最后变成:\n",
    "    # \"这是一个段落\"\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac02fa9e-c64a-48a6-9088-144ed80c2331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_item(item):\n",
    "    \"\"\"处理单条数据，统一不同数据集的格式\"\"\"\n",
    "    try:\n",
    "        # 检查数据格式并提取必要字段\n",
    "        if isinstance(item, dict):\n",
    "            # 常见格式：包含title和content的字典\n",
    "            if 'title' in item and 'content' in item:\n",
    "                title = clean_text(item['title'])\n",
    "                content = clean_text(item['content'])\n",
    "            # 其他可能的格式\n",
    "            elif 'text' in item:\n",
    "                # 如果只有text字段，尝试从文本中提取标题\n",
    "                text = clean_text(item['text'])\n",
    "                lines = text.split('\\n', 1)\n",
    "                if len(lines) > 1:\n",
    "                    title, content = lines\n",
    "                else:\n",
    "                    title = \"文章\"\n",
    "                    content = text\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "        # 检查文本长度\n",
    "        if len(content) < 50 or len(content) > 10000:\n",
    "            return None\n",
    "            \n",
    "        # 构建统一的训练格式\n",
    "        conversation = {\n",
    "            \"instruction\": f\"请生成一段带有阿拉伯专业术语的文本。\\n\\n标题: {title}\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": content,\n",
    "            \"category\": item.get('labels', {}).get('pjwk_cates', \"general\")\n",
    "        }\n",
    "        \n",
    "        return conversation\n",
    "    except Exception as e:\n",
    "        print(f\"处理数据时出错: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5630bd8c-3ba5-4bba-a77a-1eac9209e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(input_path, sample_ratio=0.1):\n",
    "    \"\"\"处理单个文件并随机抽样\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    try:\n",
    "        # 读取gzip文件\n",
    "        with gzip.open(input_path, 'rt', encoding='utf-8') as f:\n",
    "            # 首先读取所有行\n",
    "            lines = f.readlines()\n",
    "            \n",
    "            # 随机抽样\n",
    "            # 确保sample_size至少为1,避免抽样失败\n",
    "            sample_size = max(1, int(len(lines) * sample_ratio))\n",
    "            # 从lines列表中随机抽取sample_size条数据\n",
    "            sampled_lines = random.sample(lines, sample_size)\n",
    "            \n",
    "            # 处理抽样的数据\n",
    "            for line in tqdm(sampled_lines, desc=f\"处理文件 {Path(input_path).name}\"):\n",
    "                try:\n",
    "                    item = json.loads(line.strip())\n",
    "                    processed_item = process_item(item)\n",
    "                    if processed_item:\n",
    "                        processed_data.append(processed_item)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"处理数据时出错: {e}\")\n",
    "                    continue\n",
    "    except Exception as e:\n",
    "        print(f\"处理文件 {input_path} 时出错: {e}\")\n",
    "    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6be6723b-4ca9-4979-95a2-9b1d88e385f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_datasets(base_dir, output_path, sample_ratio=0.25):\n",
    "    \"\"\"处理所有数据集并合并结果\"\"\"\n",
    "    all_processed_data = []\n",
    "    \n",
    "    # 递归查找所有.jsonl.gz文件\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.jsonl.gz'):\n",
    "                input_path = os.path.join(root, file)\n",
    "                print(f\"\\n处理文件: {input_path}\")\n",
    "                \n",
    "                # 处理单个文件\n",
    "                file_data = process_file(input_path, sample_ratio)\n",
    "                all_processed_data.extend(file_data)\n",
    "                \n",
    "                print(f\"从 {file} 中提取了 {len(file_data)} 条数据\")\n",
    "    \n",
    "    # 保存所有处理后的数据\n",
    "    output_path = Path(output_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_processed_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\n处理完成！\")\n",
    "    print(f\"总共处理了 {len(all_processed_data)} 条数据\")\n",
    "    print(f\"数据已保存至: {output_path}\")\n",
    "    \n",
    "    # 输出数据集统计信息\n",
    "    categories = {}\n",
    "    for item in all_processed_data:\n",
    "        cat = item['category']\n",
    "        if isinstance(cat, dict):\n",
    "            cat = str(cat)\n",
    "        categories[cat] = categories.get(cat, 0) + 1\n",
    "    \n",
    "    print(\"\\n数据集类别分布:\")\n",
    "    for cat, count in categories.items():\n",
    "        print(f\"{cat}: {count} 条\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00c0ac70-d1d4-4c7d-aae8-75b6f8ce1ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 设置随机种子以确保可重复性\n",
    "    random.seed(42)\n",
    "    \n",
    "    # 输入输出路径\n",
    "    base_dir = \"/gemini/data-1/\"  # 原始数据集目录\n",
    "    output_file = \"/gemini/code/combined_processed_data_arabic.json\"  # 输出文件路径\n",
    "\n",
    "    # 处理所有数据集\n",
    "    process_all_datasets(base_dir, output_file, sample_ratio=0.25)\n",
    "\n",
    "    # 显示样例数据\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        print(\"\\n处理后的数据样例:\")\n",
    "        print(json.dumps(data[0], ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0961f0a-8ede-4d63-9840-1a3c75775e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "处理文件: /gemini/data-1/阿拉伯part-677f75d865d8-001143.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理文件 阿拉伯part-677f75d865d8-001143.jsonl.gz: 100%|██████████| 569725/569725 [00:08<00:00, 69539.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从 阿拉伯part-677f75d865d8-001143.jsonl.gz 中提取了 20976 条数据\n",
      "\n",
      "处理完成！\n",
      "总共处理了 20976 条数据\n",
      "数据已保存至: /gemini/code/combined_processed_data_arabic.json\n",
      "\n",
      "数据集类别分布:\n",
      "{'level1': ['professional_field'], 'level2': ['technology']}: 10707 条\n",
      "{'level1': ['professional_field'], 'level2': ['finance']}: 2881 条\n",
      "{'level1': ['professional_field'], 'level2': ['law']}: 124 条\n",
      "{'level1': ['professional_field'], 'level2': ['academic']}: 6182 条\n",
      "{'level1': ['professional_field'], 'level2': ['patent']}: 619 条\n",
      "{'level1': ['professional_field'], 'level2': ['institutions']}: 371 条\n",
      "{'level1': ['professional_field'], 'level2': ['education']}: 92 条\n",
      "\n",
      "处理后的数据样例:\n",
      "{\n",
      "  \"instruction\": \"请生成一段带有阿拉伯专业术语的文本。\\n\\n标题: آخر الأخبار: ifor\",\n",
      "  \"input\": \"\",\n",
      "  \"output\": \"الأخبار التقنية أعلنت اليوم شركة إنفور، الرائدة في مجال توريد تطبيقات المؤسسية المتخصصة والقائمة على السحابة، عن مشاركتها واستعراضها لعروض حزمة CloudSuite…\",\n",
      "  \"category\": {\n",
      "    \"level1\": [\n",
      "      \"professional_field\"\n",
      "    ],\n",
      "    \"level2\": [\n",
      "      \"technology\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97379577-4027-45f9-9dbb-7d697b2fd1ab",
   "metadata": {},
   "source": [
    "# 提取专业术语：如“操作系统”、“高血压”\n",
    "注意事项：先撇去阿拉伯语语种的特性影响，如音节，再进行分词，这样的分词效果更好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26f5a323-c676-4a8a-83d8-229a99d2b91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain_terms_by_language(text: str, lang: str) -> List[str]:\n",
    "    \"\"\"使用语言专属工具提取领域术语\"\"\"\n",
    "    terms = []\n",
    "    if lang == 'ar':\n",
    "        try:\n",
    "            # 使用pyarabic进行基本处理\n",
    "            text = araby.strip_tashkeel(text)  # 移除变音符号\n",
    "            words = araby.tokenize(text)\n",
    "            # 提取可能的术语（长度大于3的词）\n",
    "            terms = [w for w in words if len(w) > 3 and not any(c.isdigit() for c in w)]\n",
    "        except Exception as e:\n",
    "            print(f\"阿拉伯语处理失败: {e}\")\n",
    "    \n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34b9dda6-f848-4a6e-a7fb-e6e8b1cb6a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain_terms(domain_texts: List[str], general_texts: List[str] = None, top_n: int = 2000) -> List[str]:\n",
    "    \"\"\"使用改进的多语言分词方法提取领域术语\"\"\"\n",
    "    print(\"开始提取领域术语...\")\n",
    "    \n",
    "    # 按语言分组处理文本\n",
    "    lang_texts = defaultdict(list)\n",
    "    for text in domain_texts:\n",
    "        lang = detect_language(text)\n",
    "        if lang != 'unknown':\n",
    "            lang_texts[lang].append(text)\n",
    "    \n",
    "    print(\"\\n文本语言分布:\")\n",
    "    for lang, texts in lang_texts.items():\n",
    "        print(f\"- {lang}: {len(texts)} 条\")\n",
    "    \n",
    "    # 分语言处理并提取术语\n",
    "    all_terms = []\n",
    "    for lang, texts in lang_texts.items():\n",
    "        print(f\"\\n处理{lang}语言文本...\")\n",
    "        \n",
    "        # 使用语言专属工具提取术语\n",
    "        lang_terms = set()\n",
    "        for text in tqdm(texts, desc=f\"处理{lang}语言文本\"):\n",
    "            terms = extract_domain_terms_by_language(text, lang)\n",
    "            lang_terms.update(terms)\n",
    "        \n",
    "        # 过滤和排序术语\n",
    "        lang_terms = list(lang_terms)\n",
    "        # 计算每个术语在文本中出现的频率\n",
    "        # 1. 遍历每个文本\n",
    "        # 2. 对每个文本重新提取术语\n",
    "        # 3. 使用Counter统计所有术语的频率\n",
    "        term_freq = Counter(t for text in texts for t in extract_domain_terms_by_language(text, lang))\n",
    "        \n",
    "        # 对术语列表进行排序:\n",
    "        # 1. 首要排序依据是术语出现频率(term_freq[x])\n",
    "        # 2. 次要排序依据是术语长度(len(x))\n",
    "        # reverse=True表示按降序排列,即频率高的和长度长的排在前面\n",
    "        lang_terms.sort(key=lambda x: (term_freq[x], len(x)), reverse=True)\n",
    "        \n",
    "        # 根据语言数量平均分配术语数量配额\n",
    "        # 1. top_n是总的期望术语数量\n",
    "        # 2. len(lang_texts)是语言种类数\n",
    "        # 3. 对每种语言,只取配额内的高频长术语\n",
    "        # //是整除运算符,用于计算每种语言分配的术语数量配额\n",
    "        # 例如:如果top_n=2000,有4种语言,则每种语言分配2000//4=500个术语\n",
    "        # 这里的:是切片操作符,表示从列表开头取到指定位置\n",
    "        # //是整除运算符,例如10//3=3\n",
    "        # 所以lang_terms[:top_n // len(lang_texts)]表示:\n",
    "        # 1. 先计算top_n除以语言数量的整除结果n\n",
    "        # 2. 然后从lang_terms列表中取前n个元素\n",
    "        selected_terms = lang_terms[:top_n // len(lang_texts)]\n",
    "        all_terms.extend(selected_terms)\n",
    "        \n",
    "        print(f\"{lang}语言提取了 {len(selected_terms)} 个术语\")\n",
    "        if selected_terms:\n",
    "            print(f\"{lang}语言术语示例:\")\n",
    "            for term in selected_terms[:5]:\n",
    "                print(f\"  - {term}\")\n",
    "    \n",
    "    return all_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657ba914-3177-41b7-b438-50e068ff927e",
   "metadata": {},
   "source": [
    "# 数据集构建\n",
    "- 将原始数据转换为指令微调格式\n",
    "- 训练集和验证集划分\n",
    "- 数据格式标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ca5c498-07a7-4b52-bc37-06ffb36da851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(json_path, tokenizer, max_length=512, val_ratio=0.1):\n",
    "    \"\"\"加载并预处理微调数据（标准LLM微调流程）\"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        raw_data = json.load(f)\n",
    "    \n",
    "    # 记录每个样本中\"### Response:\\n\"的字符位置\n",
    "    formatted_texts = []\n",
    "    response_char_positions = []\n",
    "    for item in raw_data:\n",
    "        prompt = item['prompt']\n",
    "        output = item['output']\n",
    "        formatted_text = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n{output}\"\n",
    "        formatted_texts.append(formatted_text)\n",
    "        # 找到\"### Response:\\n\"的字符位置\n",
    "        response_start_char = formatted_text.find(\"### Response:\\n\") + len(\"### Response:\\n\")\n",
    "        response_char_positions.append(response_start_char)\n",
    "    \n",
    "    # 分词处理\n",
    "    tokenized_data = tokenizer(\n",
    "        formatted_texts,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",  # 修改：使用max_length填充\n",
    "        add_special_tokens=True,\n",
    "        return_offsets_mapping=True  # 需要字符到token的映射\n",
    "    )\n",
    "    \n",
    "    # 准备标签\n",
    "    labels = []\n",
    "    for i in range(len(tokenized_data[\"input_ids\"])):\n",
    "        input_ids = tokenized_data[\"input_ids\"][i]\n",
    "        offsets = tokenized_data[\"offset_mapping\"][i]\n",
    "        \n",
    "        # 通过字符位置找到response起始的token索引\n",
    "        response_start_token = None\n",
    "        for token_idx, (char_start, char_end) in enumerate(offsets):\n",
    "            if char_start >= response_char_positions[i]:\n",
    "                response_start_token = token_idx\n",
    "                break\n",
    "        \n",
    "        # 如果未找到（例如被截断），则设为整个序列\n",
    "        if response_start_token is None:\n",
    "            response_start_token = len(input_ids)\n",
    "        \n",
    "        # 创建label：只保留response部分的token_id\n",
    "        label = [-100] * len(input_ids)\n",
    "        label[response_start_token:] = input_ids[response_start_token:]\n",
    "        labels.append(label)\n",
    "    \n",
    "    # 移除offset_mapping（不再需要）\n",
    "    tokenized_data.pop(\"offset_mapping\")\n",
    "    \n",
    "    # 创建数据集\n",
    "    dataset = Dataset.from_dict({\n",
    "        \"input_ids\": tokenized_data[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized_data[\"attention_mask\"],\n",
    "        \"labels\": labels\n",
    "    })\n",
    "    \n",
    "    # 划分训练验证集\n",
    "    split_dataset = dataset.train_test_split(test_size=val_ratio, seed=42)\n",
    "    \n",
    "    # 转换为PyTorch张量格式\n",
    "    def set_format(ds):\n",
    "        ds.set_format(type='torch', columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "        return ds\n",
    "    \n",
    "    train_dataset = set_format(split_dataset[\"train\"])\n",
    "    val_dataset = set_format(split_dataset[\"test\"])\n",
    "    \n",
    "    # 修正后的collate_fn - 确保所有序列长度一致\n",
    "    def collate_fn(batch):\n",
    "        # 将batch中的每个tensor转为列表以便处理\n",
    "        input_ids = [item[\"input_ids\"].tolist() for item in batch]\n",
    "        attention_mask = [item[\"attention_mask\"].tolist() for item in batch]\n",
    "        labels = [item[\"labels\"].tolist() for item in batch]\n",
    "        \n",
    "        # 找出最大长度\n",
    "        max_len = max(len(ids) for ids in input_ids)\n",
    "        \n",
    "        # 进行右侧填充\n",
    "        for i in range(len(batch)):\n",
    "            pad_len = max_len - len(input_ids[i])\n",
    "            if pad_len > 0:\n",
    "                input_ids[i] += [tokenizer.pad_token_id] * pad_len\n",
    "                attention_mask[i] += [0] * pad_len\n",
    "                labels[i] += [-100] * pad_len  # 使用-100填充标签\n",
    "        \n",
    "        # 转换回tensor\n",
    "        result = {\n",
    "            \"input_ids\": torch.tensor(input_ids),\n",
    "            \"attention_mask\": torch.tensor(attention_mask),\n",
    "            \"labels\": torch.tensor(labels)\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # 创建DataLoader\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=4,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1b0035-a59f-4268-9149-7d83ebb5ba06",
   "metadata": {},
   "source": [
    "# 编码：数据处理成适合MLM的格式\n",
    "\n",
    "- 功能描述\n",
    "    该函数用于加载和预处理用于大语言模型微调的数据。它将原始JSON格式的训练数据转换为适合模型训练的格式,并创建训练和验证数据加载器。\n",
    "\n",
    "- 参数说明\n",
    "    - json_path (str): 输入JSON文件的路径\n",
    "    - tokenizer: 用于文本分词的tokenizer对象\n",
    "    - max_length (int): 序列的最大长度,默认为512\n",
    "    - val_ratio (float): 验证集比例,默认为0.1\n",
    "-处理流程\n",
    "    1. 数据加载与格式化\n",
    "\n",
    "        - 从JSON文件加载原始数据\n",
    "        - 将每个样本格式化为\"### Instruction:\\n{prompt}\\n\\n### Response:\\n{output}\"的形式\n",
    "        - 记录每个样本中\"### Response:\\n\"的字符位置\n",
    "    2. 分词处理\n",
    "\n",
    "        - 使用tokenizer对文本进行分词\n",
    "        - 设置最大长度并进行截断\n",
    "        - 使用max_length进行填充\n",
    "        - 获取字符到token的映射信息\n",
    "    3. 标签准备\n",
    "\n",
    "        - 通过字符位置找到response起始的token索引\n",
    "        - 创建标签序列:\n",
    "            - response之前的token标记为-100(忽略)\n",
    "            - response部分保留原token_id\n",
    "    4. 数据集创建与划分\n",
    "\n",
    "        - 使用Huggingface的Dataset类创建数据集\n",
    "        - 按照设定比例划分训练集和验证集\n",
    "        - 将数据格式转换为PyTorch张量\n",
    "    5. DataLoader创建\n",
    "\n",
    "        - 实现collate_fn确保batch中序列长度一致\n",
    "        - 创建训练和验证数据加载器\n",
    "        - 设置batch_size为4\n",
    "- 返回值\n",
    "返回一个元组,包含:\n",
    "\n",
    "  - train_dataloader: 训练数据加载器\n",
    "  - val_dataloader: 验证数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f786af0-9628-4f6a-b34d-e422cc8ef4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_and_prepare_data(json_path, tokenizer, max_length=512, val_ratio=0.1):\n",
    "    \"\"\"加载并预处理微调数据（标准LLM微调流程）\"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        raw_data = json.load(f)\n",
    "    \n",
    "    # 记录每个样本中\"### Response:\\n\"的字符位置\n",
    "    formatted_texts = []\n",
    "    response_char_positions = []\n",
    "    for item in raw_data:\n",
    "        prompt = item['prompt']\n",
    "        output = item['output']\n",
    "        formatted_text = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n{output}\"\n",
    "        formatted_texts.append(formatted_text)\n",
    "        # 找到\"### Response:\\n\"的字符位置\n",
    "        response_start_char = formatted_text.find(\"### Response:\\n\") + len(\"### Response:\\n\")\n",
    "        response_char_positions.append(response_start_char)\n",
    "    \n",
    "    # 分词处理\n",
    "    tokenized_data = tokenizer(\n",
    "        formatted_texts,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",  # 修改：使用max_length填充\n",
    "        add_special_tokens=True,\n",
    "        return_offsets_mapping=True  # 需要字符到token的映射\n",
    "    )\n",
    "    \n",
    "    # 准备标签\n",
    "    labels = []\n",
    "    for i in range(len(tokenized_data[\"input_ids\"])):\n",
    "        input_ids = tokenized_data[\"input_ids\"][i]\n",
    "        offsets = tokenized_data[\"offset_mapping\"][i]\n",
    "        \n",
    "        # 通过字符位置找到response起始的token索引\n",
    "        response_start_token = None\n",
    "        for token_idx, (char_start, char_end) in enumerate(offsets):\n",
    "            if char_start >= response_char_positions[i]:\n",
    "                response_start_token = token_idx\n",
    "                break\n",
    "        \n",
    "        # 如果未找到（例如被截断），则设为整个序列\n",
    "        if response_start_token is None:\n",
    "            response_start_token = len(input_ids)\n",
    "        \n",
    "        # 创建label：只保留response部分的token_id\n",
    "        label = [-100] * len(input_ids)\n",
    "        label[response_start_token:] = input_ids[response_start_token:]\n",
    "        labels.append(label)\n",
    "    \n",
    "    # 移除offset_mapping（不再需要）\n",
    "    tokenized_data.pop(\"offset_mapping\")\n",
    "    \n",
    "    # 创建数据集\n",
    "    dataset = Dataset.from_dict({\n",
    "        \"input_ids\": tokenized_data[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized_data[\"attention_mask\"],\n",
    "        \"labels\": labels\n",
    "    })\n",
    "    \n",
    "    # 划分训练验证集\n",
    "    split_dataset = dataset.train_test_split(test_size=val_ratio, seed=42)\n",
    "    \n",
    "    # 转换为PyTorch张量格式\n",
    "    def set_format(ds):\n",
    "        ds.set_format(type='torch', columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "        return ds\n",
    "    \n",
    "    train_dataset = set_format(split_dataset[\"train\"])\n",
    "    val_dataset = set_format(split_dataset[\"test\"])\n",
    "    \n",
    "    # 修正后的collate_fn - 确保所有序列长度一致\n",
    "    def collate_fn(batch):\n",
    "        # 将batch中的每个tensor转为列表以便处理\n",
    "        input_ids = [item[\"input_ids\"].tolist() for item in batch]\n",
    "        attention_mask = [item[\"attention_mask\"].tolist() for item in batch]\n",
    "        labels = [item[\"labels\"].tolist() for item in batch]\n",
    "        \n",
    "        # 找出最大长度\n",
    "        max_len = max(len(ids) for ids in input_ids)\n",
    "        \n",
    "        # 进行右侧填充\n",
    "        for i in range(len(batch)):\n",
    "            pad_len = max_len - len(input_ids[i])\n",
    "            if pad_len > 0:\n",
    "                input_ids[i] += [tokenizer.pad_token_id] * pad_len\n",
    "                attention_mask[i] += [0] * pad_len\n",
    "                labels[i] += [-100] * pad_len  # 使用-100填充标签\n",
    "        \n",
    "        # 转换回tensor\n",
    "        result = {\n",
    "            \"input_ids\": torch.tensor(input_ids),\n",
    "            \"attention_mask\": torch.tensor(attention_mask),\n",
    "            \"labels\": torch.tensor(labels)\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # 创建DataLoader\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=4,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b4b45a-bff9-4284-84ff-38a905dc0595",
   "metadata": {},
   "source": [
    "# 模型加载与训练——deepseek1.5B&开始训练\n",
    "1. 模型准备\n",
    "    - 加载DeepSeek基础模型\n",
    "    - 配置tokenizer\n",
    "    - 设置LoRA参数\n",
    "2. 训练配置\n",
    "    - 学习率设置\n",
    "    - 批次大小选择\n",
    "    - 训练轮次确定\n",
    "    - 优化器选择\n",
    "3. 训练过程\n",
    "    - 梯度更新\n",
    "    - 学习率调度\n",
    "    - 模型保存\n",
    "    - 训练监控"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df51116c-becc-4a1e-9f59-dc793b8f4b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置日志\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(f'training_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58ce3baf-9a3a-4a98-86cd-b81489d1c7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子以确保结果可重现\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a623ea9a-f582-48a3-9da3-1a74c1bcec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DomainEvaluator:\n",
    "#     def __init__(self, tokenizer, device):\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.device = device\n",
    "#         # 加载sentence transformer用于计算文本相似度\n",
    "#         self.sentence_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "#         self.sentence_model.to(device)\n",
    "        \n",
    "#         # 加载领域术语\n",
    "#         with open(\"/home/mw/input/Arabic314891489/domain_terms_arabic.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#             self.domain_terms = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "#     def calculate_domain_perplexity(self, model, eval_dataloader):\n",
    "#         \"\"\"计算领域数据的困惑度\"\"\"\n",
    "#         model.eval()\n",
    "#         total_loss = 0\n",
    "#         total_tokens = 0\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for batch in eval_dataloader:\n",
    "#                 batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "#                 outputs = model(**batch)\n",
    "#                 total_loss += outputs.loss.item() * batch[\"input_ids\"].size(0)\n",
    "#                 total_tokens += batch[\"input_ids\"].ne(self.tokenizer.pad_token_id).sum().item()\n",
    "        \n",
    "#         return torch.exp(torch.tensor(total_loss / total_tokens))\n",
    "    \n",
    "#     def evaluate_domain_adaptation(self, model, texts, lang=None):\n",
    "#         \"\"\"评估生成文本的领域适应性\"\"\"\n",
    "#         model.eval()\n",
    "#         metrics = {\n",
    "#             \"term_coverage\": [],\n",
    "#             \"term_density\": [],\n",
    "#             \"response_quality\": []\n",
    "#         }\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for text in texts:\n",
    "#                 # 构建提示\n",
    "#                 if lang:\n",
    "#                     prompt = f\"请用{lang}语言回答以下问题：\\n{text}\"\n",
    "#                 else:\n",
    "#                     prompt = text\n",
    "                \n",
    "#                 # 生成回复\n",
    "#                 inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n",
    "#                 outputs = model.generate(\n",
    "#                     **inputs,\n",
    "#                     max_length=800,\n",
    "#                     num_return_sequences=1,\n",
    "#                     do_sample=True,\n",
    "#                     temperature=0.7,\n",
    "#                     top_p=0.95\n",
    "#                 )\n",
    "#                 response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                \n",
    "#                 # 计算评估指标\n",
    "#                 term_count = sum(1 for term in self.domain_terms if term.lower() in response.lower())\n",
    "#                 term_coverage = term_count / len(self.domain_terms) if self.domain_terms else 0\n",
    "#                 metrics[\"term_coverage\"].append(term_coverage)\n",
    "                \n",
    "#                 term_density = term_count / len(response) if response else 0\n",
    "#                 metrics[\"term_density\"].append(term_density)\n",
    "                \n",
    "#                 # 计算回复质量\n",
    "#                 response_embedding = self.sentence_model.encode([response])\n",
    "#                 prompt_embedding = self.sentence_model.encode([text])\n",
    "#                 similarity = cosine_similarity(response_embedding, prompt_embedding)[0][0]\n",
    "#                 metrics[\"response_quality\"].append(similarity)\n",
    "        \n",
    "#         return {\n",
    "#             \"avg_term_coverage\": np.mean(metrics[\"term_coverage\"]),\n",
    "#             \"avg_term_density\": np.mean(metrics[\"term_density\"]),\n",
    "#             \"avg_response_quality\": np.mean(metrics[\"response_quality\"])\n",
    "#         }\n",
    "\n",
    "# import json\n",
    "# from datasets import Dataset\n",
    "# from transformers import AutoTokenizer\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# def load_and_prepare_data(json_path, tokenizer, max_length=512, val_ratio=0.1):\n",
    "#     \"\"\"加载并预处理微调数据（标准LLM微调流程）\"\"\"\n",
    "#     with open(json_path, 'r', encoding='utf-8') as f:\n",
    "#         raw_data = json.load(f)\n",
    "    \n",
    "#     # 记录每个样本中\"### Response:\\n\"的字符位置\n",
    "#     formatted_texts = []\n",
    "#     response_char_positions = []\n",
    "#     for item in raw_data:\n",
    "#         prompt = item['prompt']\n",
    "#         output = item['output']\n",
    "#         formatted_text = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n{output}\"\n",
    "#         formatted_texts.append(formatted_text)\n",
    "#         # 找到\"### Response:\\n\"的字符位置\n",
    "#         response_start_char = formatted_text.find(\"### Response:\\n\") + len(\"### Response:\\n\")\n",
    "#         response_char_positions.append(response_start_char)\n",
    "    \n",
    "#     # 分词处理\n",
    "#     tokenized_data = tokenizer(\n",
    "#         formatted_texts,\n",
    "#         max_length=max_length,\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\",  # 修改：使用max_length填充\n",
    "#         add_special_tokens=True,\n",
    "#         return_offsets_mapping=True  # 需要字符到token的映射\n",
    "#     )\n",
    "    \n",
    "#     # 准备标签\n",
    "#     labels = []\n",
    "#     for i in range(len(tokenized_data[\"input_ids\"])):\n",
    "#         input_ids = tokenized_data[\"input_ids\"][i]\n",
    "#         offsets = tokenized_data[\"offset_mapping\"][i]\n",
    "        \n",
    "#         # 通过字符位置找到response起始的token索引\n",
    "#         response_start_token = None\n",
    "#         for token_idx, (char_start, char_end) in enumerate(offsets):\n",
    "#             if char_start >= response_char_positions[i]:\n",
    "#                 response_start_token = token_idx\n",
    "#                 break\n",
    "        \n",
    "#         # 如果未找到（例如被截断），则设为整个序列\n",
    "#         if response_start_token is None:\n",
    "#             response_start_token = len(input_ids)\n",
    "        \n",
    "#         # 创建label：只保留response部分的token_id\n",
    "#         label = [-100] * len(input_ids)\n",
    "#         label[response_start_token:] = input_ids[response_start_token:]\n",
    "#         labels.append(label)\n",
    "    \n",
    "#     # 移除offset_mapping（不再需要）\n",
    "#     tokenized_data.pop(\"offset_mapping\")\n",
    "    \n",
    "#     # 创建数据集\n",
    "#     dataset = Dataset.from_dict({\n",
    "#         \"input_ids\": tokenized_data[\"input_ids\"],\n",
    "#         \"attention_mask\": tokenized_data[\"attention_mask\"],\n",
    "#         \"labels\": labels\n",
    "#     })\n",
    "    \n",
    "#     # 划分训练验证集\n",
    "#     split_dataset = dataset.train_test_split(test_size=val_ratio, seed=42)\n",
    "    \n",
    "#     # 转换为PyTorch张量格式\n",
    "#     def set_format(ds):\n",
    "#         ds.set_format(type='torch', columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "#         return ds\n",
    "    \n",
    "#     train_dataset = set_format(split_dataset[\"train\"])\n",
    "#     val_dataset = set_format(split_dataset[\"test\"])\n",
    "    \n",
    "#     # 修正后的collate_fn - 确保所有序列长度一致\n",
    "#     def collate_fn(batch):\n",
    "#         # 将batch中的每个tensor转为列表以便处理\n",
    "#         input_ids = [item[\"input_ids\"].tolist() for item in batch]\n",
    "#         attention_mask = [item[\"attention_mask\"].tolist() for item in batch]\n",
    "#         labels = [item[\"labels\"].tolist() for item in batch]\n",
    "        \n",
    "#         # 找出最大长度\n",
    "#         max_len = max(len(ids) for ids in input_ids)\n",
    "        \n",
    "#         # 进行右侧填充\n",
    "#         for i in range(len(batch)):\n",
    "#             pad_len = max_len - len(input_ids[i])\n",
    "#             if pad_len > 0:\n",
    "#                 input_ids[i] += [tokenizer.pad_token_id] * pad_len\n",
    "#                 attention_mask[i] += [0] * pad_len\n",
    "#                 labels[i] += [-100] * pad_len  # 使用-100填充标签\n",
    "        \n",
    "#         # 转换回tensor\n",
    "#         result = {\n",
    "#             \"input_ids\": torch.tensor(input_ids),\n",
    "#             \"attention_mask\": torch.tensor(attention_mask),\n",
    "#             \"labels\": torch.tensor(labels)\n",
    "#         }\n",
    "        \n",
    "#         return result\n",
    "    \n",
    "#     # 创建DataLoader\n",
    "#     train_dataloader = DataLoader(\n",
    "#         train_dataset,\n",
    "#         batch_size=4,\n",
    "#         shuffle=True,\n",
    "#         collate_fn=collate_fn\n",
    "#     )\n",
    "#     val_dataloader = DataLoader(\n",
    "#         val_dataset,\n",
    "#         batch_size=4,\n",
    "#         collate_fn=collate_fn\n",
    "#     )\n",
    "    \n",
    "#     return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d193732-6a20-4910-b422-adc9d50f422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_lora_config(model, train_dataloader, val_dataloader, device, evaluator):\n",
    "    \"\"\"搜索最优的LoRA配置，使用小数据集快速搜索\"\"\"\n",
    "    configs = [\n",
    "        {\"r\": 16, \"alpha\": 64},\n",
    "        {\"r\": 8, \"alpha\": 32},\n",
    "        {\"r\": 4, \"alpha\": 16}\n",
    "    ]\n",
    "    \n",
    "    best_perplexity = float('inf')\n",
    "    best_config = None\n",
    "    \n",
    "    # 从训练集和验证集中各取100条数据创建小数据集\n",
    "    small_train_data = []\n",
    "    small_val_data = []\n",
    "    \n",
    "    # 收集小训练集\n",
    "    train_iter = iter(train_dataloader)\n",
    "    for _ in range(min(25, len(train_dataloader))):  # 25个batch * 4 = 100条数据\n",
    "        try:\n",
    "            batch = next(train_iter)\n",
    "            small_train_data.append(batch)\n",
    "        except StopIteration:\n",
    "            break\n",
    "    \n",
    "    # 收集小验证集\n",
    "    val_iter = iter(val_dataloader)\n",
    "    for _ in range(min(25, len(val_dataloader))):\n",
    "        try:\n",
    "            batch = next(val_iter)\n",
    "            small_val_data.append(batch)\n",
    "        except StopIteration:\n",
    "            break\n",
    "    \n",
    "    logger.info(f\"创建了小数据集用于配置搜索：训练集 {len(small_train_data)} 批次，验证集 {len(small_val_data)} 批次\")\n",
    "    \n",
    "    for config in configs:\n",
    "        logger.info(f\"\\n测试LoRA配置: r={config['r']}, alpha={config['alpha']}\")\n",
    "        \n",
    "        # 配置LoRA\n",
    "        lora_config = LoraConfig(\n",
    "            r=config['r'],\n",
    "            lora_alpha=config['alpha'],\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.CAUSAL_LM\n",
    "        )\n",
    "        \n",
    "        # 创建PEFT模型\n",
    "        peft_model = get_peft_model(model, lora_config)\n",
    "        \n",
    "        # 快速训练和评估\n",
    "        optimizer = AdamW(peft_model.parameters(), lr=2e-4)\n",
    "        peft_model.train()\n",
    "        \n",
    "        # 在小训练集上训练\n",
    "        for _ in range(2):  # 只训练2个epoch\n",
    "            for batch in small_train_data:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = peft_model(**batch)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "        # 在小验证集上评估\n",
    "        peft_model.eval()\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in small_val_data:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = peft_model(**batch)\n",
    "                total_loss += outputs.loss.item() * batch[\"input_ids\"].size(0)\n",
    "                total_tokens += batch[\"input_ids\"].ne(evaluator.tokenizer.pad_token_id).sum().item()\n",
    "        # 计算困惑度，原理是计算验证集的损失除以验证集的总token数量，然后取指数，目的是衡量模型的困惑度\n",
    "        perplexity = torch.exp(torch.tensor(total_loss / total_tokens))\n",
    "        logger.info(f\"配置性能 - 困惑度: {perplexity:.2f}\")\n",
    "        \n",
    "        if perplexity < best_perplexity:\n",
    "            best_perplexity = perplexity\n",
    "            best_config = config\n",
    "            logger.info(f\"找到新的最佳配置！\")\n",
    "    \n",
    "    return best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9378d49c-b4a0-4735-90c8-91d8e0e9a8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_metrics_to_json_serializable(metrics):\n",
    "    \"\"\"将指标转换为JSON可序列化的格式\"\"\"\n",
    "    if isinstance(metrics, dict):\n",
    "        return {k: convert_metrics_to_json_serializable(v) for k, v in metrics.items()}\n",
    "    elif isinstance(metrics, list):\n",
    "        return [convert_metrics_to_json_serializable(v) for v in metrics]\n",
    "    elif isinstance(metrics, (torch.Tensor, np.ndarray)):\n",
    "        return metrics.item() if metrics.size == 1 else metrics.tolist()\n",
    "    elif isinstance(metrics, (int, float, str, bool)):\n",
    "        return metrics\n",
    "    elif metrics is None:\n",
    "        return None\n",
    "    else:\n",
    "        return str(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28a86660-7f76-45e5-8d72-0b2bb7f98fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 设置随机种子\n",
    "    set_seed(42)\n",
    "    \n",
    "    # 检查CUDA\n",
    "    assert torch.cuda.is_available(), \"需要CUDA支持\"\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    \n",
    "    print(os.environ.get('HF_ENDPOINT'))\n",
    "    print(os.environ.get('HF_ENDPOINT'))\n",
    "    print(os.environ.get('HF_ENDPOINT'))\n",
    "\n",
    "    # 加载模型和分词器\n",
    "    model_path = \"/gemini/pretrain/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "    ).to(device)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path,\n",
    "        trust_remote_code=True,\n",
    "    \n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    train_dataloader, val_dataloader = load_and_prepare_data(\"/gemini/data-1/lora_training_data_arabic.json\", tokenizer)\n",
    "    \n",
    "\n",
    "    # 初始化评估器\n",
    "    evaluator = DomainEvaluator(tokenizer, device)\n",
    "    \n",
    "    # # 寻找最优LoRA配置\n",
    "    # logger.info(\"开始寻找最优LoRA配置...\")\n",
    "    # best_config = find_optimal_lora_config(model, train_dataloader, val_dataloader, device, evaluator)\n",
    "    # logger.info(f\"找到最优LoRA配置: r={best_config['r']}, alpha={best_config['alpha']}\")\n",
    "    \n",
    "    # # 使用最优配置创建LoRA模型\n",
    "    # lora_config = LoraConfig(\n",
    "    #     r=best_config['r'],\n",
    "    #     lora_alpha=best_config['alpha'],\n",
    "    #     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    #     lora_dropout=0.1,\n",
    "    #     bias=\"none\",\n",
    "    #     task_type=TaskType.CAUSAL_LM\n",
    "    # )\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=4,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # 训练配置\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "    num_epochs = 3\n",
    "    num_training_steps = len(train_dataloader) * num_epochs\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=200,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    # 从domain_terms_arabic.txt构建评估提示\n",
    "    with open(\"/gemini/data-1/domain_terms_arabic.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        domain_terms = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    # 构建基于领域术语的评估提示\n",
    "    unlabeled_eval_prompts = {\n",
    "        \"ar\": [\n",
    "            f\"请解释阿拉伯语中'{term}'这个术语的含义和用法。\" for term in random.sample(domain_terms, 5)\n",
    "        ] + [\n",
    "            f\"请用阿拉伯语写一段话，包含以下术语：{', '.join(random.sample(domain_terms, 3))}\",\n",
    "            f\"在技术领域中，'{random.choice(domain_terms)}'和'{random.choice(domain_terms)}'这两个术语有什么联系？\",\n",
    "            f\"请用阿拉伯语描述'{random.choice(domain_terms)}'在现代技术发展中的应用。\",\n",
    "            f\"请生成一段带有阿拉伯专业术语的文本\",\n",
    "            f\"请生成一段带有阿拉伯专业术语的文本\",\n",
    "            f\"请生成一段带有阿拉伯专业术语的文本\",\n",
    "            f\"请生成一段带有阿拉伯专业术语的文本\",\n",
    "            f\"请生成一段带有阿拉伯专业术语的文本\",\n",
    "            f\"请生成一段带有阿拉伯专业术语的文本\",\n",
    "            f\"请生成一段带有阿拉伯专业术语的文本\",\n",
    "            f\"请生成一段带有阿拉伯专业术语的文本\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # 训练循环\n",
    "    best_metrics = {\n",
    "        \"val_perplexity\": float('inf'),\n",
    "        \"domain_adaptation\": 0\n",
    "    }\n",
    "    metrics_log = []\n",
    "    eval_steps = 100  # 每300步评估一次\n",
    "    best_model_path = \"/gemini/Arabic314891489/deepseek-lora-best\"\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "        print(len(train_dataloader))\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "            \n",
    "            # 评估\n",
    "            if step % eval_steps == 0 and step > 0:\n",
    "                model.eval()\n",
    "                \n",
    "                # 1. 计算验证集困惑度\n",
    "                val_perplexity = evaluator.calculate_domain_perplexity(model, val_dataloader)\n",
    "                \n",
    "                # 2. 评估领域适应性\n",
    "                domain_metrics = {}\n",
    "                for lang, prompts in unlabeled_eval_prompts.items():\n",
    "                    metrics = evaluator.evaluate_domain_adaptation(model, prompts, lang)\n",
    "                    domain_metrics[lang] = metrics\n",
    "                \n",
    "                # 3. 计算综合指标\n",
    "                avg_domain_score = np.mean([\n",
    "                    m[\"avg_term_coverage\"] * 0.4 +\n",
    "                    m[\"avg_term_density\"] * 0.3 +\n",
    "                    m[\"avg_response_quality\"] * 0.3\n",
    "                    for m in domain_metrics.values()\n",
    "                ])\n",
    "                \n",
    "                # 记录当前学习率\n",
    "                current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "                \n",
    "                metrics = {\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"step\": step,\n",
    "                    \"val_perplexity\": val_perplexity.item() if isinstance(val_perplexity, torch.Tensor) else float(val_perplexity),\n",
    "                    \"domain_adaptation_score\": float(avg_domain_score),\n",
    "                    \"learning_rate\": float(current_lr),\n",
    "                    \"domain_metrics\": convert_metrics_to_json_serializable(domain_metrics)\n",
    "                }\n",
    "                metrics_log.append(metrics)\n",
    "                \n",
    "                logger.info(f\"\\n验证集困惑度: {val_perplexity:.4f}\")\n",
    "                logger.info(f\"领域适应性得分: {avg_domain_score:.4f}\")\n",
    "                logger.info(f\"当前学习率: {current_lr:.6f}\")\n",
    "                \n",
    "                # 4. 保存最佳模型\n",
    "                combined_score = avg_domain_score/val_perplexity\n",
    "                if combined_score > best_metrics[\"domain_adaptation\"]/best_metrics[\"val_perplexity\"]:\n",
    "                    best_metrics[\"val_perplexity\"] = float(val_perplexity)\n",
    "                    best_metrics[\"domain_adaptation\"] = float(avg_domain_score)\n",
    "                    model.save_pretrained(best_model_path)\n",
    "                    logger.info(f\"保存新的最佳模型！困惑度={val_perplexity:.4f}, 领域得分={avg_domain_score:.4f}\")\n",
    "                \n",
    "                model.train()\n",
    "\n",
    "        # 每个epoch结束保存检查点\n",
    "        checkpoint_path = f\"/gemini/Arabic314891489/deepseek-lora-checkpoint-{epoch+1}\"\n",
    "        try:\n",
    "            model.save_pretrained(checkpoint_path)\n",
    "            logger.info(f\"已保存Epoch {epoch+1}检查点\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"保存检查点时出错: {e}\")\n",
    "        \n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        logger.info(f\"Epoch {epoch+1} 平均损失: {avg_loss:.4f}\")\n",
    "    \n",
    "    # 保存最终模型和训练指标\n",
    "    try:\n",
    "        model.save_pretrained(\"/gemini/Arabic314891489/deepseek-lora-final\")\n",
    "        with open('training_metrics.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(metrics_log, f, ensure_ascii=False, indent=2)\n",
    "        logger.info(\"训练完成！已保存最终模型和训练指标\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"保存最终结果时出错: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "816c100a-7145-4fe4-bc6c-610fdbbc6781",
   "metadata": {},
   "outputs": [],
   "source": [
    "19. 主程序入口\n",
    "if __name__ == \"__main__\":\n",
    "    # 打印环境信息\n",
    "    logger.info(f\"PyTorch version: {torch.__version__}\")\n",
    "    logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    logger.info(f\"CUDA version: {torch.version.cuda}\")\n",
    "    logger.info(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    logger.info(f\"Current GPU: {torch.cuda.current_device()}\")\n",
    "    logger.info(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dce94d8-f0ef-4136-9bcb-ce70bc877daf",
   "metadata": {},
   "source": [
    "# 模型合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a99361bb-4b0b-4fa3-bcad-361cbaa0af57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# 设置日志\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(f'merge_and_save_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89395062-a619-4a9a-94bb-a5b0ae742d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_metrics_to_json_serializable(metrics):\n",
    "    \"\"\"将指标转换为JSON可序列化的格式\"\"\"\n",
    "    if isinstance(metrics, dict):\n",
    "        return {k: convert_metrics_to_json_serializable(v) for k, v in metrics.items()}\n",
    "    elif isinstance(metrics, list):\n",
    "        return [convert_metrics_to_json_serializable(v) for v in metrics]\n",
    "    elif isinstance(metrics, (torch.Tensor, np.ndarray)):\n",
    "        return metrics.item() if metrics.size == 1 else metrics.tolist()\n",
    "    elif isinstance(metrics, (int, float, str, bool)):\n",
    "        return metrics\n",
    "    elif metrics is None:\n",
    "        return None\n",
    "    else:\n",
    "        return str(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51e3e277-2b2d-418c-8af1-3a7e4d5c784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics_and_merge_model():\n",
    "    \"\"\"保存训练指标并合并模型\"\"\"\n",
    "    try:\n",
    "        # 1. 保存训练指标（如果有）\n",
    "        if 'metrics_log' in globals():\n",
    "            logger.info(\"正在保存训练指标...\")\n",
    "            metrics_log_serializable = [convert_metrics_to_json_serializable(m) for m in metrics_log]\n",
    "            with open('training_metrics.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(metrics_log_serializable, f, ensure_ascii=False, indent=2)\n",
    "            logger.info(\"训练指标已保存到 training_metrics.json\")\n",
    "        \n",
    "        # 2. 加载和合并模型\n",
    "        logger.info(\"开始加载模型...\")\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # 加载基础模型\n",
    "        base_model_path = \"/gemini/pretrain/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "        logger.info(f\"加载基础模型: {base_model_path}\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_path,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16\n",
    "        ).to(device)\n",
    "        \n",
    "        # 加载tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            base_model_path,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # 配置LoRA\n",
    "        lora_config = LoraConfig(\n",
    "            r=4,\n",
    "            lora_alpha=16,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.CAUSAL_LM\n",
    "        )\n",
    "        \n",
    "        # 创建PEFT模型\n",
    "        logger.info(\"应用LoRA配置...\")\n",
    "        model = get_peft_model(base_model, lora_config)\n",
    "        \n",
    "        # 加载最佳检查点\n",
    "        best_model_path = \"/gemini/pretrain2/deepseek-lora-best\"\n",
    "        logger.info(f\"加载最佳检查点: {best_model_path}\")\n",
    "        model.load_adapter(best_model_path, adapter_name=\"arabic_adapter\")\n",
    "        \n",
    "        # 合并权重\n",
    "        logger.info(\"合并模型权重...\")\n",
    "        merged_model = model.merge_and_unload()\n",
    "        \n",
    "        # 保存合并后的模型\n",
    "        merged_model_path = \"/gemini/Arabic314891489/deepseek-merged\"\n",
    "        logger.info(f\"保存合并后的模型到: {merged_model_path}\")\n",
    "        merged_model.save_pretrained(merged_model_path)\n",
    "        \n",
    "        # 保存tokenizer\n",
    "        logger.info(\"保存tokenizer...\")\n",
    "        tokenizer.save_pretrained(merged_model_path)\n",
    "        \n",
    "        logger.info(\"所有操作完成！\")\n",
    "        \n",
    "        # 返回模型和tokenizer以供后续使用\n",
    "        return merged_model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"处理过程中出错: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3303f72-5975-4b3c-b1d1-2848c698257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_merged_model(model, tokenizer, test_prompts=None):\n",
    "    \"\"\"测试合并后的模型\"\"\"\n",
    "    if test_prompts is None:\n",
    "        # 使用domain_terms.txt中的术语构建测试提示\n",
    "        try:\n",
    "            with open(\"/gemini/data-1/domain_terms_arabic.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "                domain_terms = [line.strip() for line in f if line.strip()]\n",
    "            \n",
    "            # 随机选择一些术语构建测试提示\n",
    "            import random\n",
    "            selected_terms = random.sample(domain_terms, min(5, len(domain_terms)))\n",
    "            test_prompts = [\n",
    "                f\"请解释阿拉伯语中'{term}'这个术语的含义。\" for term in selected_terms\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"无法加载domain_terms.txt，使用默认测试提示: {e}\")\n",
    "            test_prompts = [\n",
    "                f\"请用阿拉伯语写一段话，包含以下术语：{', '.join(random.sample(domain_terms, 3))}\",\n",
    "                f\"在技术领域中，'{random.choice(domain_terms)}'和'{random.choice(domain_terms)}'这两个术语有什么联系？\",\n",
    "                f\"请用阿拉伯语描述'{random.choice(domain_terms)}'在现代技术发展中的应用。\",\n",
    "                f\"请生成一段带有阿拉伯专业术语的文本\",\n",
    "                f\"请生成一段带有阿拉伯专业术语的文本\",\n",
    "                f\"请生成一段带有阿拉伯专业术语的文本\",\n",
    "                f\"请生成一段带有阿拉伯专业术语的文本\",\n",
    "                f\"请生成一段带有阿拉伯专业术语的文本\",\n",
    "                f\"请生成一段带有阿拉伯专业术语的文本\",\n",
    "                f\"请生成一段带有阿拉伯专业术语的文本\",\n",
    "                f\"请生成一段带有阿拉伯专业术语的文本\"\n",
    "            ]\n",
    "    \n",
    "    logger.info(\"\\n开始测试合并后的模型...\")\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    for prompt in test_prompts:\n",
    "        logger.info(f\"\\n测试提示: {prompt}\")\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=512,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                top_p=0.95,\n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        logger.info(f\"模型回答: {response}\\n\")\n",
    "        logger.info(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6510a32c-15f8-4ff1-8cfc-f46355836717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:PyTorch version: 2.7.0+cu126\n",
      "INFO:__main__:CUDA available: True\n",
      "INFO:__main__:CUDA version: 12.6\n",
      "INFO:__main__:GPU count: 1\n",
      "INFO:__main__:Current GPU: 0\n",
      "INFO:__main__:GPU name: B1.gpu.large\n",
      "INFO:__main__:开始加载模型...\n",
      "INFO:__main__:加载基础模型: /gemini/pretrain/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "INFO:__main__:应用LoRA配置...\n",
      "INFO:__main__:加载最佳检查点: /gemini/pretrain2/deepseek-lora-best\n",
      "INFO:__main__:合并模型权重...\n",
      "INFO:__main__:保存合并后的模型到: /gemini/Arabic314891489/deepseek-merged\n",
      "INFO:__main__:保存tokenizer...\n",
      "INFO:__main__:所有操作完成！\n",
      "INFO:__main__:\n",
      "开始测试合并后的模型...\n",
      "INFO:__main__:\n",
      "测试提示: 请解释阿拉伯语中'الأولى'这个术语的含义。\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "INFO:__main__:模型回答: 请解释阿拉伯语中'الأولى'这个术语的含义。在什么情况下使用这个词？\n",
      "</think>\n",
      "\n",
      "在这个问题中，\"أولى\" 是阿拉伯语中一个常见的短语，意思是“最早”或“最先”。它通常用来表示相对于其他事物来说最早的，或者作为时间的起点。\n",
      "\n",
      "例如：\n",
      "- 如果我们说：“أولى الصراع”，意思是指最早发生的事件。\n",
      "- 如果我们在谈论时间，我们会用“أولى”来指代过去最开始的时间点。\n",
      "\n",
      "有时候，“أولى”也可以用于强调某种事物的优先级或重要性。具体使用场景取决于上下文和表达的意思。\n",
      "\n",
      "INFO:__main__:--------------------------------------------------\n",
      "INFO:__main__:\n",
      "测试提示: 请解释阿拉伯语中'Galaxy'这个术语的含义。\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "INFO:__main__:模型回答: 请解释阿拉伯语中'Galaxy'这个术语的含义。在翻译时要使用中文，同时尽量简明扼要地表达。\n",
      "\n",
      "---\n",
      "\n",
      "请回答问题：Galaxy是英文中的一个词汇，用来描述星系。我刚刚学习了阿拉伯语，想了解“Galaxy”这个词的阿拉伯语翻译，但不知道具体怎么翻译。请给出正确的翻译方式，并说明其在阿拉伯语中表示的意义。\n",
      "</think>\n",
      "\n",
      "在阿拉伯语中，“Galaxy”的翻译是 \"السمر\" (Marram) 或 \"السمر المكمل\" (Marram al-muhut). \n",
      "\n",
      "\"السمر\" (Marram) 通常指的是“星系”，类似于英文中的“Galaxy”。在阿拉伯语中，“السمر”有时也可以被用于更广泛的“天体系统”（Star System）或“宇宙结构”（Cosmological Structure）的称呼。\n",
      "\n",
      "INFO:__main__:--------------------------------------------------\n",
      "INFO:__main__:\n",
      "测试提示: 请解释阿拉伯语中'الملكية'这个术语的含义。\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "INFO:__main__:模型回答: 请解释阿拉伯语中'الملكية'这个术语的含义。为什么用户会想到用这个术语来描述一个网页标题？请详细说明，包括使用例子。\n",
      "\n",
      "阿拉伯语中的“الملكية”这个词在日常生活中有多种含义，它通常意味着权力、 dominion 或 superior authority。但在网络用语或网站标题中，这个词更常被用来表达一种隐晦的、可能暗示某种控制权或权威性的东西。例如，在一些社交媒体平台如Twitter上，当有人提到某个账号的“الملكية”时，他们实际上是在强调该账号有能力操纵或影响力很大。这种用法常常用于吸引读者的注意，并鼓励他们去查看该账号的内容，因为它被认为能够提供有价值的信息或观点。\n",
      "\n",
      "总结来说，“الملكية”在这些情况下不仅仅是一个正常的词汇，而是象征着一种潜在的力量和影响力。因此，当一个人想用这个词作为网页标题时，他们实际上是在创造一种带有隐晦意义的标题，以吸引更多的点击和互动。\n",
      "</think>\n",
      "\n",
      "在阿拉伯语中，“الملكية”（Al-Khwarizmi）指的是“智慧者”或“贤者”。然而，这个词在实际语言中多指权力、支配或统治。但在网络用语中，尤其是网页标题中，这个词有时被赋予隐晦的意义，暗示某种控制力或影响力。\n",
      "\n",
      "### 1. 在网络中的用法\n",
      "在某些社交媒体平台上，如Twitter，人们常用“الملكية”来形容账号的能力。例如：\n",
      "- “我有一个‘الملكية’很强的账号。”\n",
      "这表明该账号具有很大的影响力或控制权。\n",
      "\n",
      "### 2. 示例\n",
      "一个网页标题可能像：\n",
      "- “我是‘الملكية’强的！”\n",
      "\n",
      "### 3. 总结\n",
      "使用“الملكية”作为网页标题时，作者试图传达账号的强大能力或影响力，吸引点击并鼓励阅读内容。\n",
      "\n",
      "INFO:__main__:--------------------------------------------------\n",
      "INFO:__main__:\n",
      "测试提示: 请解释阿拉伯语中'المعلومات'这个术语的含义。\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "INFO:__main__:模型回答: 请解释阿拉伯语中'المعلومات'这个术语的含义。请用简明扼要的语言，不使用任何表情符号。\n",
      "</think>\n",
      "\n",
      "\"الم眼界\"中的\"الم眼界\"（Maliq）是一个阿拉伯语词汇，通常用于描述富有的个人或家族。这个词源自阿拉伯语中的“ال mim”（mali），意为“高贵”或“财富”，因此被用来形容那些拥有高收入和地位的人物。\n",
      "\n",
      "在现代英语中，“mali”常用于指代那些有特权、地位较高的个体，如企业家或显赫的个人。\n",
      "\n",
      "INFO:__main__:--------------------------------------------------\n",
      "INFO:__main__:\n",
      "测试提示: 请解释阿拉伯语中'نائب'这个术语的含义。\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "INFO:__main__:模型回答: 请解释阿拉伯语中'نائب'这个术语的含义。此外，请详细说明其在英文中的翻译以及相关用例。\n",
      "</think>\n",
      "\n",
      "阿拉伯语中的“نائب”（n’aiz）是一个常见的词汇，通常用于描述某种现象或事物。它的意思是“因缘果报”，即由过去的历史和环境所造成的结果。这个术语在中文里常用来表达一种被动的生命态度，认为人生不如意只是因为之前经历的种种原因。\n",
      "\n",
      "### 算法：关于“نائب”的翻译和用例\n",
      "在英语中，“nafs”是“n’aiz”的常见同义词。它通常用来表达被动的生命态度，如“我感到沮丧，这主要是因为我过去的经历”。\n",
      "\n",
      "**例句：**\n",
      "1. She has a nafs of success due to her hard work.\n",
      "   - 她的成功源自她的努力。\n",
      "2. The nafs of happiness is when you live life with joy and gratitude.\n",
      "   - 生活幸福的主要来源是快乐与感恩。\n",
      "3. His nafs of wisdom comes from years of learning.\n",
      "   - 他的智慧来自于数年的学习。\n",
      "\n",
      "这些例子展示了“nafs”在不同语境下的使用方式。\n",
      "\n",
      "INFO:__main__:--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 打印环境信息\n",
    "    logger.info(f\"PyTorch version: {torch.__version__}\")\n",
    "    logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"CUDA version: {torch.version.cuda}\")\n",
    "        logger.info(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "        logger.info(f\"Current GPU: {torch.cuda.current_device()}\")\n",
    "        logger.info(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # 执行合并和保存\n",
    "    merged_model, tokenizer = save_metrics_and_merge_model()\n",
    "    \n",
    "    # 测试模型\n",
    "    test_merged_model(merged_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f740aec-df6b-4d10-8d80-25574dbe29c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载模型: /gemini/Arabic314891489/deepseek-merged\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提示: ['请生成一段带有阿拉伯专业术语的阿拉伯语种文本', '请生成一段带有阿拉伯专业术语的阿拉伯语种文本', '请生成一段带有阿拉伯专业术语的阿拉伯语种文本', '请生成一段带有阿拉伯专业术语的阿拉伯语种文本']\n",
      "\n",
      "生成的回复:\n",
      "--------------------------------------------------\n",
      "请生成一段带有阿拉伯专业术语的阿拉伯语种文本，主题是关于概率论，包含“条件期望”和“联合分布”的概念。\n",
      "\n",
      "请确保内容准确无误，符合学术规范，并且语言使用正确。\n",
      "好的，我现在需要帮用户生成一段带有阿拉伯专业术语的阿拉伯语文本。主题是概率论，包含“条件期望”和“联合分布”。首先，我得确认用户的需求是什么。看起来他们可能在做相关的研究或教学，所以内容必须准确、符合学术规范。\n",
      "\n",
      "接下来，我会考虑用户的深层需求。他们可能不仅想要一个段落，还希望这段文字能够清晰传达理论知识，帮助别人理解这些概念。因此，我需要用专业但易懂的语言来表达，确保术语准确，比如“ال condicion expected”（条件期望）和“ال joint distribution”（联合分布）。\n",
      "\n",
      "然后，我要检查语法和用词是否正确，避免任何错误。阿拉伯语中的某些词汇可能会有特定的发音或者拼写，所以我需要确保没有遗漏任何细节。同时，保持句子结构流畅，逻辑清晰，让读者容易理解和掌握。\n",
      "\n",
      "最后，我会通读整个文本，确保所有要点都涵盖了：定义条件期望和联合分布，以及它们之间的关系，并且说明如何从联合分布导出条件期望。这样用户就能得到一份完整且准确的材料了。\n",
      "</think>\n",
      "\n",
      "ال condicion expected (Conditional Expectation) و توزيع م joint (Joint Distribution) في احتمالات (Probability) تُعرف بان توزيع م joint يعتمد على قيم المتغيرات والجودة، بينما تُعرف بان توزيع م conditional يعتمد على قيم المتغيرات المذكورة. \n",
      "\n",
      "من الناحية التالية، تُعرف بان توزيع م joint ي encapsulates All possible relationships between Variables, while تُعرف بان توزيع م conditional تallows نشر probabilities based on Given Values of Variables.\n",
      "\n",
      "لذا، تُعرف بان توزيع م joint ي serving كا لغة base لفهم م jointness，在 Probability Theory 中至关重要。\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载合并后的模型和tokenizer\n",
    "model_path = \"/gemini/Arabic314891489/deepseek-merged\"\n",
    "print(f\"正在加载模型: {model_path}\")\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 设置生成参数\n",
    "generation_config = {\n",
    "    \"max_length\": 800,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.95,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"num_return_sequences\": 1\n",
    "}\n",
    "\n",
    "# 构建提示\n",
    "prompts = [\"请生成一段带有阿拉伯专业术语的阿拉伯语种文本\",\"请生成一段带有阿拉伯专业术语的阿拉伯语种文本\",\"请生成一段带有阿拉伯专业术语的阿拉伯语种文本\",\"请生成一段带有阿拉伯专业术语的阿拉伯语种文本\"]\n",
    "print(f\"提示: {prompts}\")\n",
    "\n",
    "# 对提示进行编码\n",
    "inputs = tokenizer(prompts, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# 生成文本\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        **generation_config\n",
    "    )\n",
    "\n",
    "# 解码并打印结果\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\n生成的回复:\")\n",
    "print(\"-\" * 50)\n",
    "print(response)\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1745175-adfd-4b4a-aeb6-6767e71d42c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载模型: /gemini/Arabic314891489/deepseek-merged\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提示: 请生成一段带有阿拉伯专业术语的阿拉伯语种文本\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "生成的回复:\n",
      "--------------------------------------------------\n",
      "请生成一段带有阿拉伯专业术语的阿拉伯语种文本，内容涉及数学或物理领域。\n",
      "</think>\n",
      "\n",
      "السلام عليكم！  \n",
      "مرحباً في هذا اليوم، شكرًا لمساعدتك في تعلم有任何 شيء من التراث والتنبيه.  \n",
      "\n",
      "للحصول على معلومات أكثر تفصيلاً في مجالات مثلMathematics und Physics، يُنصح بمتابعة النسخة الموثوقة وتعزيز الفهم بالتفاصيل التي ت用户提供ها.  \n",
      "\n",
      " best regards,  \n",
      "[您的姓名]\n",
      "--------------------------------------------------\n",
      "提示: 请生成一段带有阿拉伯专业术语的阿拉伯语种文本\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "生成的回复:\n",
      "--------------------------------------------------\n",
      "请生成一段带有阿拉伯专业术语的阿拉伯语种文本，用于学术研究或教育用途。\n",
      "</think>\n",
      "\n",
      "\"السياق البارد\" (Bar扎i's Market)  \n",
      "في عالم التحديات العينية، تُعتبر القيمة البارزة في حب العيد والضيوفة.  \n",
      "\n",
      "الاستماعات الإخبارية (Exchange Rates) الباردية من الم markets البارдية يساعد على دراسة التحديات العينية وال Dinamic effects in exchange rates.\n",
      "--------------------------------------------------\n",
      "提示: 请生成一段带有阿拉伯专业术语的阿拉伯语种文本\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "生成的回复:\n",
      "--------------------------------------------------\n",
      "请生成一段带有阿拉伯专业术语的阿拉伯语种文本，主题是关于概率和统计学在人工智能中的应用。需要注意的是，要确保技术准确，且内容涵盖关键点如分布、模型评估等。\n",
      "\n",
      "请从以下选项中选择合适的语言：\n",
      "1. 中文\n",
      "2. 阿拉伯语（阿拉伯语）\n",
      "3. 英文\n",
      "\n",
      "请给出具体的回答。\n",
      "</think>\n",
      "\n",
      "Sure! Here's a translated text:\n",
      "\n",
      "**Probability and Statistics in Artificial Intelligence: A Comprehensive Overview**\n",
      "\n",
      "In the realm of artificial intelligence, probability and statistics play pivotal roles in shaping the foundation for machine learning models and deep learning algorithms. These mathematical disciplines enable developers to design robust systems that can process vast amounts of data efficiently.\n",
      "\n",
      "One key area where probability and statistics are applied is in probabilistic graphical models, which help in understanding complex relationships between variables within a system. Additionally, statistical methods like hypothesis testing are crucial in validating assumptions and ensuring the reliability of AI models.\n",
      "\n",
      "Model evaluation is another critical aspect. Techniques such as cross-validation and Bayesian inference are employed to assess how well an AI model performs on unseen data. This ensures that models not only fit historical data well but also generalize effectively to new inputs.\n",
      "\n",
      "Overall, the integration of probability and statistics into AI is essential for advancing technologies such as natural language processing, computer vision, and robotics. By leveraging these tools, researchers and practitioners continue to push the boundaries of what AI can achieve.\n",
      "--------------------------------------------------\n",
      "提示: 请生成一段带有阿拉伯专业术语的阿拉伯语种文本\n",
      "\n",
      "生成的回复:\n",
      "--------------------------------------------------\n",
      "请生成一段带有阿拉伯专业术语的阿拉伯语种文本，包含以下内容：\n",
      "1. 一元二次方程。\n",
      "2. 等式和不等式的性质。\n",
      "3. 方程求解过程中的移项步骤。\n",
      "\n",
      "希望这段文本能够准确描述这些数学概念，并且在翻译成英文时保持准确性。特别注意对涉及数字或特殊符号的部分进行正确的标点表示。\n",
      "\n",
      "请确保语言表达准确，并且格式正确。\n",
      "\n",
      "首先，请用阿拉伯语撰写这个段落。然后，将该阿拉伯语段落翻译成英文，确保所有数学符号和表达方式都正确无误。\n",
      "\n",
      "请仔细阅读并理解每个部分的要求，避免遗漏任何细节。尤其是涉及数学公式的部分，必须准确无误地呈现出来。\n",
      "好的，我需要根据用户的要求生成一个阿拉伯语文本和对应的英文翻译。首先，我会仔细阅读用户的指示，确保完全理解每一个要点。\n",
      "\n",
      "首先，用户要求生成一段阿拉伯语文本，其中包括三个主要内容：一元二次方程、等式和不等式的性质，以及方程求解过程中的移项步骤。接下来，用户还强调了要将这段阿拉伯语翻译成英文，并保留准确性，特别是在涉及数字或特殊符号的地方，要用正确的标点表示。\n",
      "\n",
      "那么，我先来思考如何构建这段阿拉伯语文本。首先，介绍一元二次方程的概念，可能需要用阿拉伯语的代词，如“الynomials”或者“ال variate”。然后，讨论等式和不等式的性质，可能需要用到诸如“equality”、“inequality”这样的词汇。接着，详细描述方程求解过程中移项的具体步骤，比如在移项后如何调整变量的位置以求解未知数。\n",
      "\n",
      "接下来是翻译过程。将这段阿拉伯语转化为英文时，需要确保数学符号和术语的准确性。例如，“algebraic equation”应该译为“algebraic equation”，而“quadratic equation”应译为“quadratic equation”。此外，标点符号如句号和逗号也需要使用正确的格式，尤其是在涉及到多行句子的情况下。\n",
      "\n",
      "现在，我开始构思具体的阿拉伯语内容。首先是介绍一元二次方程：\n",
      "\n",
      "\"الإحصاء ال::$x^{2}$+$2x$+$1=0$، حيث $x$ هو المتغير، و$1$ هو الحد.\"\n",
      "\n",
      "这里，“::$”用于分隔方程两边，但实际在阿拉伯语中，通常使用“:)”来表示分隔线，因此可能需要更准确地处理这一点。同时，“$”表示美元符号，但在数学表达中可能不需要如此严格地分割，所以这里可能需要调整符号的显示方式。\n",
      "\n",
      "接下来，讨论等式和不等式的性质：\n",
      "\n",
      "\"ال$x^{2} + 2x + 1 = 0$، الذي يشير إلى أن $x^{2}$ added to $2x$ plus $1$ equals zero.\"\n",
      "\n",
      "这里的“$\\Rightarrow$”表示逻辑推导，但在这个上下文中可能更适合用“$\\rightarrow$”来表示函数关系，这样更符合数学表达习惯。\n",
      "\n",
      "最后，描述方程求解过程中的移项步骤：\n",
      "\n",
      "\"لاباسًا $1$ من كلا sides، obtaining $x^{2} + 2x = -1$. This process involves moving the constant term to the other side of the equation, adjusting variables accordingly to solve for $x$.\"\n",
      "\n",
      "其中，“$\\Rightarrow$”表示从“$x^{2} + 2x + 1 = 0$”到“$x^{2} + 2x = -1$”的过程，这可能是因为“$\\rightarrow$”更适合此情况。\n",
      "\n",
      "接下来，翻译这段阿拉伯语文本成英文，确保准确性。首先，处理阿拉伯语中的术语和符号：\n",
      "\n",
      "- “Algebraic Equation” → “Algebraic Equation”\n",
      "-\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 加载合并后的模型和tokenizer\n",
    "model_path = \"/gemini/Arabic314891489/deepseek-merged\"\n",
    "print(f\"正在加载模型: {model_path}\")\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 设置生成参数\n",
    "generation_config = {\n",
    "    \"max_length\": 800,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.95,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"num_return_sequences\": 1\n",
    "}\n",
    "\n",
    "prompts = [\"请生成一段带有阿拉伯专业术语的阿拉伯语种文本\",\n",
    "          \"请生成一段带有阿拉伯专业术语的阿拉伯语种文本\",\n",
    "          \"请生成一段带有阿拉伯专业术语的阿拉伯语种文本\",\n",
    "          \"请生成一段带有阿拉伯专业术语的阿拉伯语种文本\"]\n",
    "\n",
    "for prompt_text in prompts:\n",
    "    print(f\"提示: {prompt_text}\")\n",
    "    # 对提示进行编码\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # 生成文本\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            **generation_config\n",
    "        )\n",
    "\n",
    "    # 解码并打印结果\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"\\n生成的回复:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(response)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac5f3c-18d2-49cc-ad8f-51fc19c1c078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "# import time\n",
    "\n",
    "# # 定义源目录和目标目录\n",
    "# source_dir = \"/home/mw/input\"\n",
    "# target_dir = \"/home/mw/project\"\n",
    "\n",
    "# # 确保目标目录存在\n",
    "# os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# print(f\"开始从 {source_dir} 复制文件到 {target_dir}...\")\n",
    "\n",
    "# # 获取源目录中所有内容\n",
    "# all_items = []\n",
    "# for root, dirs, files in os.walk(source_dir):\n",
    "#     # 为每个目录创建相应的目标目录\n",
    "#     for dir_name in dirs:\n",
    "#         src_path = os.path.join(root, dir_name)\n",
    "#         rel_path = os.path.relpath(src_path, source_dir)\n",
    "#         dst_path = os.path.join(target_dir, rel_path)\n",
    "#         os.makedirs(dst_path, exist_ok=True)\n",
    "    \n",
    "#     # 收集所有文件\n",
    "#     for file_name in files:\n",
    "#         src_file = os.path.join(root, file_name)\n",
    "#         all_items.append(src_file)\n",
    "\n",
    "# # 显示总文件数\n",
    "# total_files = len(all_items)\n",
    "# print(f\"找到 {total_files} 个文件需要复制\")\n",
    "\n",
    "# # 复制文件并显示进度\n",
    "# start_time = time.time()\n",
    "# copied_size = 0\n",
    "# copied_files = 0\n",
    "\n",
    "# for src_file in all_items:\n",
    "#     # 计算目标路径\n",
    "#     rel_path = os.path.relpath(src_file, source_dir)\n",
    "#     dst_file = os.path.join(target_dir, rel_path)\n",
    "    \n",
    "#     # 确保目标目录存在\n",
    "#     os.makedirs(os.path.dirname(dst_file), exist_ok=True)\n",
    "    \n",
    "#     # 复制文件\n",
    "#     try:\n",
    "#         file_size = os.path.getsize(src_file)\n",
    "#         shutil.copy2(src_file, dst_file)  # copy2保留元数据\n",
    "#         copied_size += file_size\n",
    "#         copied_files += 1\n",
    "        \n",
    "#         # 每5个文件显示一次进度\n",
    "#         if copied_files % 5 == 0 or copied_files == total_files:\n",
    "#             elapsed = time.time() - start_time\n",
    "#             percent = copied_files / total_files * 100\n",
    "#             size_mb = copied_size / (1024**2)\n",
    "#             print(f\"进度: {copied_files}/{total_files} ({percent:.1f}%) - 已复制 {size_mb:.2f} MB - 用时 {elapsed:.1f} 秒\")\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"复制 {src_file} 时出错: {e}\")\n",
    "\n",
    "# # 显示完成信息\n",
    "# elapsed_time = time.time() - start_time\n",
    "# copied_gb = copied_size / (1024**3)  # 转换为GB\n",
    "\n",
    "# print(f\"\\n复制完成!\")\n",
    "# print(f\"总共复制了 {copied_files} 个文件，{copied_gb:.2f} GB\")\n",
    "# print(f\"耗时: {elapsed_time:.2f} 秒，平均速度: {copied_gb/elapsed_time:.2f} GB/秒\")\n",
    "\n",
    "# # 验证复制是否成功\n",
    "# source_file_count = sum([len(files) for _, _, files in os.walk(source_dir)])\n",
    "# target_file_count = sum([len(files) for _, _, files in os.walk(target_dir)])\n",
    "\n",
    "# print(f\"\\n验证结果:\")\n",
    "# print(f\"源目录文件数: {source_file_count}\")\n",
    "# print(f\"目标目录文件数: {target_file_count}\")\n",
    "# print(f\"{'复制成功' if source_file_count == target_file_count else '复制可能不完整'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f4d8f9-fef5-4516-9e12-17751d7fb622",
   "metadata": {},
   "source": [
    "# 总结\n",
    "本课程我们深入学习了大模型微调的全链条内容\n",
    "\n",
    "数据集准备与清洗\n",
    "编码\n",
    "确定评估指标与训练方法\n",
    "模型下载（部署）与训练\n",
    "模型评估\n",
    "\n",
    "数据集准备与清洗\n",
    "我们通过opendatalab提供的原始数据，完成了“专业名词提取+IO数据对”的构造，IO数据对的格式满足\"### Instruction:\\n{prompt}\\n\\n### Response:\\n{output}\"\n",
    "正确的数据格式才能带来有效的微调效果\n",
    "\n",
    "编码\n",
    "deepseek作为经典的因果语言，我们将数据集编码成符合因果语言训练的格式，该环节最终输出单向编码的数据，单向编码可以让模型在训练过程中学习训练集中的上下文语义关系\n",
    "\n",
    "确定评估指标与训练方法\n",
    "我们通过困惑度（perplexity）与 领域适应性评估（术语覆盖率、术语密度、响应质量）作为评估指标，这些指标也是LLM中常用的评估指标\n",
    "\n",
    "模型下载（部署）与训练\n",
    "我们指定了镜像环境，加快了模型下载速度，并且通过Lora微调方法在加快训练速度同时，保证训练质量\n",
    "\n",
    "模型评估\n",
    "我们对比了不同训练次数下的模型性能，“/gemini/Arabic314891489/deepseek-lora-best”权重目录下的模型性能最佳，也证明了评估指标与微调方法的有效性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665be003-bc16-4506-aa72-71c5e84d7b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
