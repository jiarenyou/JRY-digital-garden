---
title: 第11课 Sinusoidal、RoPE、ALiBi等各类位置信息编码
draft: false
tags:
  - AI
---
 
<iframe width="560" height="315" src="https://player.bilibili.com/player.html?autoplay=0&bvid=BV1ErPkeSEHn" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe>

### 目录
- [视频基本信息](#视频基本信息)
- [内容总结与提炼](#内容总结与提炼)
  - [1. 核心主题与目标](#1-核心主题与目标)
  - [2. 关键知识点梳理](#2-关键知识点梳理)
  - [3. 逻辑结构与关系](#3-逻辑结构与关系)
  - [4. 重要细节与论证](#4-重要细节与论证)
- [总结与复习](#总结与复习)
  - [1. 核心要点回顾](#1-核心要点回顾)
  - [2. 易混淆点与难点](#2-易混淆点与难点)
  - [3. 启发与思考](#3-启发与思考)
- [AI 总结](#ai-总结)

---

# Transformer 位置信息编码技术：从 Sinusoidal 到 RoPE 与 ALiBi

### 视频基本信息
*   **视频主题/标题**：【11】Sinusoidal、RoPE、ALiBi等各类位置信息编码
*   **视频来源/讲师**：羽毛
*   **视频时长**：约 33 分钟

<iframe width="560" height="315" src="https://player.bilibili.com/player.html?autoplay=0&bvid=BV1ErPkeSEHn" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe>


### 内容总结与提炼

#### 1. 核心主题与目标
*   **核心概念**: 视频深入探讨了 Transformer 模型中用于表示序列顺序的各类位置信息编码（Position Encoding）技术。
*   **关键信息**: 详细介绍了从经典的 Sinusoidal（正余弦）编码，到现代大语言模型中广泛使用的 RoPE（旋转位置编码）和 ALiBi（线性偏置注意力）等技术的原理、实现、优缺点及应用场景。
*   **学习目标**: 帮助观众理解绝对与相对位置编码的根本区别，掌握 RoPE 通过“旋转”注入位置信息的核心思想，了解 ALiBi 的高效线性偏置方法，并能够清晰地比较不同方案在性能、效率和长文本处理能力上的权衡。

#### 2. 关键知识点梳理

##### 2.1. 位置编码的分类：绝对 vs. 相对

位置编码主要分为两大类：绝对位置编码和相对位置编码。

*   **绝对位置编码 (Absolute Position Encoding)**
    *   **定义**: 为每个位置分配一个固定的、唯一的编码。例如，简单地从 1 编码到 4000。
    *   **优势**: 计算速度快，因为它是一个预先定义好的固定值，只需做加法运算。
    *   **劣势**:
        1.  **相对信息不明显**: 模型难以学习到 token 之间的动态相对位置关系。例如，位置 2 和 4 的差距，与位置 8 和 10 的差距，在模型看来是相同的，无法捕捉上下文的动态性。
        2.  **外推能力差**: 推理时的上下文窗口长度受限于训练时的长度。如果模型训练时最大长度为 4k，那么它无法处理超过 4k 的文本，因为没有见过更靠后的位置编码。

*   **相对位置编码 (Relative Position Encoding)**
    *   **定义**: 关注于 token 之间的相对距离或关系，而不是它们的绝对位置。
    *   **优势**:
        1.  **学习相对信息**: 模型能够更好地理解 token 之间的相对位置关系，这对于语言理解至关重要。
        2.  **更好的外推潜力**: 理论上对上下文长度的扩展性更好。
    *   **劣势**:
        1.  **计算量更大**: 相比绝对编码，计算通常更复杂。
        2.  **部分实现与 KV Cache 不兼容**: 某些相对位置编码方案在每一步推理时都会改变 Key 和 Value 的表示，导致无法使用 KV Cache 这一关键的推理优化技术。

##### 2.2. 正弦/余弦位置编码 (Sinusoidal Position Encoding)

这是 `Transformer` 原始论文中提出的方法，巧妙地同时具备了绝对和相对位置编码的特性。

*   **原理**: 将词嵌入向量与一个根据位置生成 sinusoidal（正弦/余弦）信号的向量进行**相加**。
    *Screenshot-[05:51]*
*   **公式**: 对于位置 `pos` 和维度 `i`，其编码计算如下：
    $$
    PE_{(pos, 2i)} = \sin(\frac{pos}{10000^{2i/d_{\text{model}}}})
    $$
    $$
    PE_{(pos, 2i+1)} = \cos(\frac{pos}{10000^{2i/d_{\text{model}}}})
    $$
*   **特性与局限性**:
    *   在低维度（`i` 较小），位置编码的频率高，变化快；在高维度，频率低，变化慢。
    *   在早期机器翻译等短文本任务中效果很好。
    *   对于需要处理超长上下文的大语言模型，其外推（extrapolation）性能表现不佳，效果会衰减。
    *Screenshot-[07:06]*

##### 2.3. 旋转位置编码 (RoPE - Rotary Position Embedding)

RoPE 是目前大语言模型（如 LLaMA、Qwen）中最主流的位置编码方案之一，因其强大的性能和外推能力而备受青睐。

*   **核心思想**: 不再将位置信息“加”到词嵌入中，而是在生成 Query (Q) 和 Key (K) 向量之后，通过**旋转**的方式将位置信息融入进去。
*   **实现机制**:
    1.  **保持原始词嵌入不变**: 不对初始的 token embedding 做任何修改。
    2.  **对 Q/K 进行旋转**: 将高维度的 Q 和 K 向量两两配对，看作一系列的二维向量。
    *Screenshot-[13:09]*
    3.  **二维旋转**: 对每一对二维向量 `(x_1, x_2)`，根据其绝对位置 `m` 和维度索引 `i`，将其旋转一个角度 `m * θ_i`。
    *Screenshot-[09:09]*
    4.  **旋转的数学原理**: 在二维平面上，将两个向量 `w_1` 和 `w_2` 旋转同一个角度，它们之间的夹角（即点积所反映的相似度）保持不变。RoPE 正是利用了这一特性。
    *Screenshot-[10:26]*
    5.  **拼接**: 将所有旋转后的二维向量重新拼接起来，形成新的、包含了位置信息的 Q 和 K 向量。

*   **公式**: 旋转角度 `θ_i` 的计算方式与 Sinusoidal 编码的频率项非常相似：
    $$
    \theta_i = 10000^{-2i/d_{\text{model}}}
    $$
    旋转操作可以由一个旋转矩阵 `R` 实现。对于位置为 `m` 的向量 `q_m`，其旋转后的形式为 `q'_m = R_m * q_m`。

*   **特性与优势**:
    *   **长文本外推能力强**: 由于旋转操作的周期性，RoPE 能够很好地处理比训练时更长的文本序列，注意力分数的衰减更平滑。
    *Screenshot-[21:12]*
    *Screenshot-[21:47]*
    *Screenshot-[22:13]*
    *   **相对位置编码**: 任意两个位置 `m` 和 `n` 的 QK 点积只与它们的相对位置 `m-n` 有关。
    *   **比喻**: 它的工作方式就像甩一根绳子，可以无限地延伸下去，而波形模式保持稳定。
    *Screenshot-[22:23]*

*   **为何只作用于 Q 和 K**:
    *   位置编码的核心目的是影响注意力分数的计算 (`QK^T`)，让模型知道哪个词与哪个词相关以及它们的位置关系。
    *   注意力分数计算完成后，会作用于 Value (V) 向量，V 向量本身代表的是 token 的内容信息，无需直接注入位置信息。

##### 2.4. YaRN (Yet another RoPE)

YaRN 是对 RoPE 的一种改进和微调技术，旨在进一步提升其在超长上下文场景下的表现。

*   **定义**: 基于 RoPE 的一种优化变体，通过一些高级技术来调整其行为。
*   **核心技术**:
    *   **NTK-aware Scaling / Dynamic NTK**: 一种神经网络理论技术，用于调整高频信息，防止在上下文扩展时信息丢失。
    *Screenshot-[25:29]*
    *   **Position Interpolation (PI)**: 位置插值。其核心思想是通过一个缩放因子 `s` 来调整位置索引 `m`，将更长的上下文“压缩”到模型原始训练的窗口范围内。
        *   公式调整：将原始 RoPE 中的位置 `m` 替换为 `m' = m / s`，其中 `s` 是新旧上下文长度的比值。这使得模型能够平滑地处理超出预训练长度的序列。
*   **应用**: 主要用于对已有的预训练模型（如 LLaMA）进行微调，以无损或低损地扩展其有效的上下文窗口，常被各大厂商（如通义千问）用于其长文本模型中。

##### 2.5. ALiBi (Attention with Linear Biases)

ALiBi 是另一种非常高效且简单的位置编码方法，在某些模型（如 MPT, BLOOM）中被采用。

*   **核心思想**: 直接在注意力分数矩阵上添加一个线性的、固定的偏置（bias）项，而不是修改 Q 或 K 向量。
*   **实现机制**:
    1.  正常计算 Q 和 K，并得到注意力分数矩阵 `S = QK^T`。
    2.  创建一个与 `S` 同样大小的偏置矩阵 `B`。
    3.  `B` 中的值 `B_ij` 与 token `i` 和 `j` 之间的距离 `|i-j|` 成正比，且为负数。即距离越远，惩罚越大。
    4.  将偏置矩阵加到注意力分数上：`S' = S + B`。
    5.  最后对 `S'` 进行 `softmax`。
    *Screenshot-[28:50]*
*   **优势**:
    *   **计算极其高效**: 只需要一次矩阵加法，计算量非常小。
    *   **窗口延展性极强**: 由于其惩罚是线性的，可以自然地外推到任意长度的上下文。
*   **劣势**:
    *   **位置信息不够精细**: 这种固定的线性惩罚可能不如 RoPE 那样能捕捉到复杂的、动态的位置关系，在某些精细的语言任务上性能可能稍弱。

##### 2.6. 各类编码方法对比

*Screenshot-[30:21]*

*   **训练与推理速度**:
    *   **最快**: ALiBi 和 Sinusoidal。因为它们的计算非常简单（加法）。
    *   **较慢**: RoPE 和 T5 相对位置编码。因为它们需要对 QK 向量进行额外的操作（旋转或更复杂的计算）。
*   **内存占用**:
    *   各方案在内存占用上差异不大。
*   **应用选择**:
    *   尽管 RoPE 在计算上稍慢，但因其出色的性能和强大的长文本能力，已成为当前许多 SOTA（State-of-the-Art）大模型的首选。
    *   ALiBi 因其卓越的效率和超长窗口延展性，在需要处理极长文本且对计算资源敏感的场景中具有优势。
    *   RoPE 技术还可以通过微调的方式，应用于原本未使用该技术的基础模型，以扩展其上下文窗口。

#### 3. 逻辑结构与关系
*   **组织结构**: 视频采用了“概念分类 -> 经典回顾 -> 核心技术详解 -> 衍生优化 -> 并列方案 -> 横向对比”的结构。
*   **内容关系**:
    1.  首先建立**绝对**与**相对**编码的宏观分类。
    2.  然后回顾经典的 **Sinusoidal** 作为基准。
    3.  重点深入讲解当前最主流的 **RoPE**，并引出其优化版 **YaRN**，构成一条技术演进线。
    4.  将 **ALiBi** 作为一个并列的、设计思路完全不同的主流方案进行介绍。
    5.  最后通过图表进行**横向对比**，清晰总结了各方案的权衡，为技术选型提供依据。

#### 4. 重要细节与论证
*   **RoPE 的旋转不变性**: 视频通过二维向量旋转后点积不变的几何直觉，生动地解释了 RoPE 的核心数学原理。
*   **RoPE 的外推能力**: 展示了 RoPE 在不同上下文长度（100, 1k, 10k）下的注意力热图，直观地证明了其在长文本场景下的稳定性和有效性。
*   **ALiBi 的简洁性**: 通过其直接在注意力分数矩阵上添加偏置的图示，清晰地展示了其实现的简单和高效。
*   **性能量化对比**: 引用了包含具体性能数据（words/sec）的图表，为不同编码方案的效率提供了客观的论证依据。

### 总结与复习

#### 1. 核心要点回顾
1.  **位置编码是关键**: 它是让 Transformer 模型理解序列顺序的核心机制，主要分为绝对和相对两大类。
2.  **RoPE 是主流**: 通过“旋转”Q/K向量来编码位置，不直接修改词嵌入，在保持相对位置信息的同时，拥有出色的长文本外推能力，是当前许多先进大模型的选择。
3.  **ALiBi 是高效选择**: 通过给注意力分数添加一个与距离成正比的“惩罚项”，实现简单高效，窗口延展性极强，但位置信息可能不如 RoPE 精细。
4.  **没有银弹**: 不同的位置编码方案是在**效果**和**效率**之间的权衡。选择哪种方案取决于具体的模型架构、任务需求和计算预算。

#### 2. 易混淆点与难点
*   **RoPE 的实现机制**: 将高维向量拆分为二维对进行旋转，初听可能较为抽象。关键在于理解这是一种将二维旋转的优良数学特性（点积不变性）巧妙应用到高维空间的工程方法。
*   **RoPE vs. Sinusoidal**:
    *   **作用对象**: Sinusoidal 直接作用于**词嵌入**（加法）。RoPE 作用于线性变换后的 **Q/K 向量**（旋转/乘法）。
*   **RoPE vs. ALiBi**:
    *   **作用阶段**: RoPE 在计算注意力分数**之前**修改 Q/K。ALiBi 在计算出注意力分数**之后**修改分数矩阵。

#### 3. 启发与思考
*   **设计哲学的演进**: 从 Sinusoidal 的显式“相加”，到 RoPE 的隐式“旋转”，再到 ALiBi 的“事后偏置”，位置编码的设计体现了对模型性能、效率和外推能力不断追求的创新过程。
*   **长文本的挑战**: 如何让模型在有限的训练长度下，学会处理无限长的真实世界文本，是位置编码技术需要解决的核心问题。RoPE 和 ALiBi 在这方面都做出了杰出贡献。
*   **未来的方向**: 未来的位置编码技术可能会更加动态和自适应，甚至可能完全由模型在训练中自主学习，摆脱固定数学公式的束缚。

---

### AI 总结
本笔记系统梳理了 Transformer 模型中几种关键的位置编码技术。首先，区分了**绝对位置编码**（计算快但外推能力差）和**相对位置编码**（能学习相对关系）两大类。接着，回顾了经典的**Sinusoidal**编码，并指出其在长文本场景下的局限性。

笔记的核心是详细解析了两种现代主流方案：
1.  **RoPE (旋转位置编码)**：通过旋转 Q/K 向量来融入位置信息，其核心优势在于既能保持 token 间的相对位置关系，又具备强大的长文本外推能力，是目前许多顶尖大模型的首选。
2.  **ALiBi (线性偏置注意力)**：一种极为高效的方法，它直接在计算出的注意力分数上添加一个与距离成正比的惩罚项。该方法计算量极小，窗口延展性极强。

最后，笔记通过对比各方案在训练/推理速度和内存上的表现，指出了它们在性能与效率之间的权衡。总而言之，RoPE 以其卓越的综合性能成为主流，而 ALiBi 则在追求极致效率和超长上下文的场景中表现突出。