---
tags:
  - 科技/AI
publish date: 2025-07-13T15:44:00
reviewed date: 
title: 什么是线性变换 Linear Transformation
source: https://www.bilibili.com/video/BV15SPke5EaH?spm_id_from=333.788.player.switch&vd_source=cff3eef3abcdb3fcf7537244dd23cb21
description: 视频通过几何直观的方式，深入浅出地解释了线性变换（Linear Transformation）的本质，并将其与 AI 大语言模型（特别是 Transformer）中的权重矩阵（如 WQ）联系起来，旨在帮助观众理解这些抽象的数学概念在 AI 模型中究竟扮演着怎样的角色。
obsidian-note-status:
  - completed
---

---
### **内容总结与提炼**

#### **核心主题与目标**
视频通过几何直观的方式，深入浅出地解释了线性变换（Linear Transformation）的本质，并将其与 AI 大语言模型（特别是 Transformer）中的权重矩阵（如 WQ）联系起来，旨在帮助观众理解这些抽象的数学概念在 AI 模型中究竟扮演着怎样的角色。

#### **逻辑结构与关系**
视频的讲解逻辑层层递进，结构清晰：
1.  **几何直观引入**：从二维坐标系入手，通过旋转、拉伸等可视化操作，建立对线性变换的直观理解。
2.  **数学原理解析**：将几何变换与矩阵乘法联系起来，解释变换矩阵是如何通过改变基向量来影响空间中任意向量的。
3.  **应用到AI模型**：将线性变换的概念推广到高维空间，并具体阐述其在 Transformer 模型中作为权重矩阵 `W` 的实际作用。
4.  **概念深化与澄清**：进一步区分了模型中两种不同权重的更新（词嵌入 vs 变换矩阵），并解释了高维到低维投影的可视化意义。

---

## **目录**
1. [线性变换的几何直观](#1-线性变换的几何直观)
2. [线性变换与矩阵乘法](#2-线性变换与矩阵乘法)
3. [Transformer 中的线性变换](#3-transformer-中的线性变换)
4. [线性变换在 AI 中的深层含义](#4-线性变换在-ai-中的深层含义)
5. [高维空间与降维投影](#5-高维空间与降维投影)
6. [模型训练中的两种权重更新](#6-模型训练中的两种权重更新)
7. [AI 总结](#ai-总结)

---

## 1. 线性变换的几何直观

### 1.1. 什么是线性变换？
线性变换（Linear Transformation）在几何上可以理解为对坐标系的一种“扭曲”，例如旋转、拉伸、剪切（Shear）等。
*Screenshot-[00:19]*

### 1.2. 线性变换的两个核心规则
为了保证变换是“线性”的，必须满足两个前提条件：
1.  **直线在变换后仍然是直线**，不能弯曲成曲线。
2.  **原点（Origin）保持固定**，且网格线在变换后保持平行且等距分布。这意味着一个 `1x1` 的方格在变换后可能变成一个平行四边形，但所有的平行四边形面积都是相等的。
*Screenshot-[01:20]*

### 1.3. 基向量 (Basis Vectors)
- 在一个标准的二维坐标系中，我们有两个**基向量**：
  - **`î`**（或视频中的 `x` 轴单位向量）： `[1, 0]`
  - **`ĵ`**（或视频中的 `y` 轴单位向量）： `[0, 1]`
- 空间中任何一个向量，例如 `[2, 2]`，都可以表示为基向量的线性组合，即 `2 * î + 2 * ĵ`。
*Screenshot-[00:33]*

## 2. 线性变换与矩阵乘法

### 2.1. 变换的本质：跟踪基向量
- 线性变换的全部信息都蕴含在**基向量变换后的位置**中。只要我们知道 `î` 和 `ĵ` 变换后的新坐标，就能确定空间中任何一个向量变换后的位置。
- **示例**：假设经过一次旋转和拉伸的变换后：
  - 原来的 `x` 轴基向量 `[1, 0]` 变到了新位置 `[1, -1]`。
  - 原来的 `y` 轴基向量 `[0, 1]` 变到了新位置 `[1, 1]`。
*Screenshot-[03:00]*

### 2.2. 构建变换矩阵
- 我们可以将变换后的基向量坐标作为列，构建一个**变换矩阵 (Transformation Matrix)**。
- 在上述例子中，变换矩阵 `M` 为：
  $$
  M = \begin{bmatrix} 1 & 1 \\ -1 & 1 \end{bmatrix}
  $$
  其中，第一列 `[1, -1]` 是 `î` 的新坐标，第二列 `[1, 1]` 是 `ĵ` 的新坐标。
*Screenshot-[03:53]*

### 2.3. 计算新向量坐标
- 要计算任意向量（如 `v = [2, 2]`）经过该变换后的新坐标，只需用变换矩阵 `M` 左乘该向量 `v`。
- **计算过程**：
  $$
  \text{v}_{\text{new}} = M \cdot \text{v}_{\text{old}} = \begin{bmatrix} 1 & 1 \\ -1 & 1 \end{bmatrix} \begin{bmatrix} 2 \\ 2 \end{bmatrix} = \begin{bmatrix} (1 \cdot 2) + (1 \cdot 2) \\ (-1 \cdot 2) + (1 \cdot 2) \end{bmatrix} = \begin{bmatrix} 4 \\ 0 \end{bmatrix}
  $$
- **结论**：原始向量 `[2, 2]` 在坐标系被“扭曲”后，其在新坐标系下的位置对应于原始坐标系中的 `[4, 0]`。
*Screenshot-[05:07]*

## 3. Transformer 中的线性变换

### 3.1. 将概念推广到高维
- 在 Transformer 模型中，词嵌入（Word Embedding）通常存在于一个高维空间，例如 `d_model = 512` 维。
- 此时，输入 `x` 是一个 `512 x 1` 的向量，代表一个词（Token）。
- 模型中的权重矩阵，如 `WQ`、`WK`、`WV`，就是高维空间中的**变换矩阵**。
*Screenshot-[06:23]*

### 3.2. 权重矩阵 WQ 的作用
- `WQ` 是一个 `512 x 512` 的方阵。
- 它的作用与二维例子中的变换矩阵完全相同：定义了一个对 512 维空间的线性变换。
- 矩阵中的每一列代表了 512 个基向量中对应一个在变换后的新坐标。
- 当我们执行 `WQ * x` 这个运算时，我们实际上是在根据 `WQ` 定义的规则，**“扭曲”整个 512 维空间**，从而将输入的词向量 `x` 移动到一个新的位置。
  - **输入 `x`**: `[512, 1]` (单个 Token) 或 `[512, seq_len]` (多个 Token)
  - **变换矩阵 `WQ`**: `[512, 512]`
  - **输出 `x_new`**: `[512, 1]` 或 `[512, seq_len]`
*Screenshot-[09:16]*
- **关键点**：线性变换只改变向量的坐标值，不改变其维度。`x` 进去是 512 维，出来还是 512 维。

## 4. 线性变换在 AI 中的深层含义

### 4.1. 上下文相关的向量“移动”
- 线性变换的根本目的，是在**特定语境 (Context) 下调整词向量的位置**，使其更好地表达语义关系。
- **例子：** “苹果” (Apple) 这个词
  - **语境1**: “我爱吃**苹果**” -> 在这个语境下，模型通过 `WQ` 矩阵，将“苹果”的向量向“食物”、“水果”等概念所在的语义空间区域移动。
  - **语境2**: “筷子兄弟的《小**苹果**》” -> 在这个语境下，`WQ` 会将“苹果”的向量向“歌曲”、“音乐”等概念所在的区域移动。
- 模型通过在不同 `Transformer Block` 中使用不同的权重矩阵 `W`，逐层地、精细地调整词向量的位置，以捕捉复杂的语义依赖。

### 4.2. 偏置项 (Bias)
- 完整的线性层公式是 `y = Wx + b`。
- `b` 是偏置项（Bias），它在几何上代表了对整个坐标系进行一次**平移 (Translation)**。
- 线性变换（`Wx`）负责旋转、拉伸和剪切，而偏置项 `b` 负责在变换后将所有向量统一移动一个固定的距离。

## 5. 高维空间与降维投影

- **高维到低维的投影**：将 512 维空间中的向量关系，映射（或“投影”）到我们能理解的二维或三维空间进行可视化。
- 这个过程会损失精度，但可以保留向量之间的**相对关系**（例如，哪些向量聚类在一起）。
- 这有助于我们直观地理解模型学到了什么，例如在“食物”语境下，“苹果”、“香蕉”、“梨”的向量会聚集在一起。
*Screenshot-[16:50]*

## 6. 模型训练中的两种权重更新

在 Transformer 模型的训练过程中，主要有两类参数在不断更新，它们扮演不同角色：

1.  **词嵌入本身 (Token Embedding, 即 `x` 的初始值)**
    - **作用**：调整词与词之间**固有**的、**不依赖于特定上下文**的语义关系。
    - **目标**：通过大量语料的学习，让本身语义相近的词（如“你好”、“Hello”）在向量空间中的初始位置就比较接近。

2.  **线性变换矩阵 (如 `WQ`, `WK`, `WV` 等)**
    - **作用**：学习如何**根据上下文**来动态调整（移动/变换）词向量。
    - **目标**：捕捉词在具体句子中的角色和关系。例如，将“苹果”根据上下文移动到“食物”或“歌曲”的区域。
*Screenshot-[19:25]*

**总结**：`Token Embedding` 决定了词的“出厂设置”，而线性变换矩阵 `W` 决定了在不同“场景”（语境）下如何对它进行“改造”。

---

## 总结
线性变换在几何上是对坐标系进行旋转、拉伸等操作，其核心在于改变基向量的位置。这一变换可以通过一个**变换矩阵**来精确描述，任何向量的新坐标都可以通过用该矩阵左乘其原始坐标得到。

在 Transformer 等 AI 模型中，权重矩阵 `W`（如 `WQ`）就是这样一个高维的线性变换矩阵。它并非一堆无意义的数字，而是定义了一套“扭曲”高维语义空间的规则。通过 `y = Wx + b` 的运算，模型能够根据特定**上下文 (Context)**，将输入的词向量 `x` 移动到语义空间中一个更合适的新位置，从而捕捉复杂的语言现象。模型训练的过程，就是同时优化词的初始位置（Embedding）和在不同语境下的移动规则（权重矩阵 `W`）。