#机器学习 #AI 
什么是损失函数

损失函数(Loss Function)是用来度量模型的预测值f(x)与真实值Y的差异程度的运算函数。它是一个非负实值函数，通常用L(Y,f(x))来表示，损失函数越小，模型的鲁棒性就越好。

用来做什么

损失函数是用在模型训练过程中，每批数据输入模型后，通过前向传播输出预测值，然后损失函数会计算真实值和预测值之间的差异，也就是损失值，模型通过反向传播去更新各个参数，来降低真实值和预测值之间的损失，使得模型生成的预测值往真实值方向靠拢，达到学习目的。

有哪些损失函数

基于距离度量的损失函数

均方误差损失函数（MSE）：回归问题中，均方误差损失函数用来度量样本点到回归线的距离。值越小，精度越好。

优点：由于无参数，计算成本低和具有明确的物理意义等优点

尽管MSE在图像和语音处理方面表现较弱，但它仍是评价信号质量的标准，在回归问题中，MSE常作为经验损失和算法的性能指标

L2损失函数

L2损失函数是均方误差开根号，L2损失又称欧氏距离，常用于度量数据点之间的相似度。由于L2损失具有凸性和可微性，且在 独立同分布的高斯噪声情况下，他能提供最大似然估计，使它成为回归问题、模式识别、图像处理中最常用的损失函数

L1损失函数

L1又称为曼哈顿距离，表示残差的绝对值之和。L1对离群点有很好的鲁棒性。

缺点一：在残差为0处不可导，缺点二：更新的梯度始终相同，

也就是说损失很小，梯度也会很大，这样不利于模型的收敛，

为了解决收敛问题

，一般的解决办法是在优化算法中使用变化的

学习率

，在损失接近最小时降低学习率

Smooth L1损失函数

Smooth L1损失是由Girshick R在Fast R-CNN中提出的，

主要用在目标检测中防止梯度爆炸

huber损失函数

huber损失是平方损失和绝对损失的综合，它克服了平方损失和绝对损失的缺点，不仅使损失函数具有连续的导数，而且利用MSE梯度随误差减小的特性，可取得更精确的最小值。尽管huber损失对异常点具有更好的鲁棒性，但是，它不仅引入了额外的参数，而且选择合适的参数比较困难，这也增加了训练和调试的工作量

基于概率分布度量的损失函数

KL散度函数（相对熵

可以度量两个概率分布之间的距离，也可以衡量两个随机分布之间的距离。

两个随机分布的相似度越高，它们的KL散度越小，当两个随机分布的差别增大时，它们的KL散度也会增大，可以用于比较文本标签或图像的相似度

交叉熵损失

交叉熵是信息论中的一个概念，最初用于估算平均编码长度，引入机器学习后，用于评估当前训练得到的概率分布与真实分布的差异情况。为了使神经网络的每一层输出从线性组合转为非线性逼近，以提高模型的预测精度，在以交叉熵为损失函数的神经网络模型中一般选用tanh、sigmoid、softmax或ReLU作为激活函数。

交叉熵损失函数刻画了实际输出概率与期望输出概率之间的相似度，也就是交叉熵的值越小，两个概率分布就越接近，特别是在正负样本不均衡的分类问题中，常用交叉熵作为损失函数。目前，交叉熵损失函数是卷积神经网络中最常使用的分类损失函数，它可以有效避免梯度消散。在二分类情况下也叫做对数损失函数。

softmax损失函数

Focal loss

如何选择损失函数

选择最能表达数据的主要特征来构建基于距离或基于概率分布度量的特征空间

选择合理的特征归一化方法，使特征向量转换后仍能保持原来数据的核芯内容

选取合理的损失函数，在实验的基础上，依据损失不断调整模型的参数，使其尽肯能实现类别区分

合理组合不同的损失函数，返回每个损失函数的有点，使他们能更好地度量样本间的相似性

将数据的主要特征嵌入损失函数，提升基于特定任务的模型预测精确度

