---
title: 特征工程
draft: false
tags:
  - 机器学习
---

# 一 特征工程的重要性和处理

## 1.1 重要性
特征工程就是从原始数据中提取特征的过程，这些特征能过够很好地描述数据，并且能够利用特征建立的模型在未知数据上的性能表现可以达到最优（或者接近最佳性能）。特征工程一般包括特征使用、特征获取、特征处理、特征选择、特征监控。
业界广泛流传着这样一句话“数据和特征决定了机器学习的上线，而模型和算法只是逼近这个上限而已”

- 特征工程特征越好，灵活性越强
	好的特征的灵活性在于允许你选择不复杂的模型，同时运行速度更快，更容易理解和维护
- 特征越好，构建的模型越简单
	好的特征，不需要花费太多的时间去寻找最有效的参数，这大大降低了模型的复杂度
- 特征越好，模型的性能越出色
	特征工程的最终目的就是提升模型的性能

## 1.2 处理流程
1. 首先去掉无用特征
2. 再去掉冗余特征，比如共线特征
3. 利用存在的特征、转换特征、内容中的特征以及其他数据源生成的新特征
4. 然后对特征进行转换（数值化，类别转换，归一化等）
5. 最后对特征进行处理（异常值，最大值，最小值，缺失值等），以符合模型的使用

简单来说，特征工程的处理一般包括数据预处理、特征工程、特征选择等工作，而特征选择视情况而定，如果特征数量较多，则可以进行特征选择等操作。


# 二 数据预处理和特征处理

## 2.1 数据预处理
在特征提取之前，要对数据进行预处理，具体包括数据采集、数据清洗、数据采样

### 2.1.1 数据采集
数据采集之前，有几个问题需要明确：
- 首先你要明确你的最终目的是什么
- 根据这个目的，来确定哪些数据对最后的预测结果有帮助，结合上前的数据情况是能采集到的
- 这些数据实时计算时数据获取是否快捷

### 2.1.2 数据清洗

机器学习的算法就是一个加工机器，至于最后的产品如何，取决于原材料的好坏。需要把原材料的脏数据去除。脏数据一般是不符合常理，根据业务经验判断。

### 2.1.3 数据采样

在数据清洗过后，正负样本时不均衡的，故要进行数据采样，采样的方法有随机采样和分层采样，随机采样可能会使得数据很不均匀，所以更多的使用分层抽样。
正负样本不平衡的处理方法：
- 正样本>负样本，且量都特别大的情况：采用下采样（downsampling）
- 正样本>负样本，且量不大的情况：可以采用上采样（oversampling），比如凸显那个识别中的镜像和旋转；损失函数设置样本权重

## 2.2 特征处理
特征处理的方法包括标准化、区间缩放、归一化、特征二值化、哑变量、缺失值处理、数据转换（多项式、指数函数、对数函数）

| 类                   | 功能        | 说明                             |
| ------------------- | --------- | ------------------------------ |
| StandardScaler      | 无量纲化      | 标准化，基于特征矩阵的列，将特征值转换为服从标准正态分布   |
| MinMaxScaler        | 无量纲化      | 区间缩放，基于最大值或最小值，将特征值转换为[0，1]区间内 |
| Normalizer          | 归一化       | 基于特征矩阵的行，将样本向量转换为单位向量          |
| Binarizer           | 定量特征二值化   | 基于给定阈值，将定量特征按阈值划分              |
| OneHotEncoder       | 定性特征哑编码   | 将定性特征编码为定量特征                   |
| Inputer             | 缺失值处理     | 计算缺失值，缺失值可填充为均值等               |
| PolynomialFeatures  | 多项式数据转换   | 多项式数据转换                        |
| FunctionTransformer | 自定义单元数据转换 | 使用单变元函数转换数据                    |

>[!sumary] MinMaxScaler和Normalizer的区别
>MinMaxScaler是基于矩阵的列进行归一化，而Normalizer是基于矩阵的行，把向量转换为单位向量，进行归一化的
>1. MinMaxScaler： MinMaxScaler是基于矩阵的列进行归一化，即将每个特征（对应矩阵的一列）缩放到指定的范围（通常是0到1之间）。这种方法保留了原始数据的形式，适用于需要保留原始数据分布形状的算法。其缺点是受异常值的影响较大，对分布不均匀的数据集可能导致信息损失 [1](https://blog.csdn.net/qq_42658739/article/details/136574905) 。
>2. 2. Normalizer： Normalizer是基于矩阵的行进行归一化，即将每个样本（对应矩阵的一行）的特征向量转换为单位向量。这种方法使得每个样本的特征向量都具有相同的尺度，目的是为了让样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准。Normalizer通过对每个特征进行线性变换，将特征值缩放到指定的最小值和最大值之间 [1](https://blog.csdn.net/qq_42658739/article/details/136574905) 。
>3. 总结：
>	MinMaxScaler主要关注特征之间的相对关系，通过线性变换将特征值缩放到指定的范围内。Normalizer主要关注样本之间的相似性或距离计算，通过调整每个样本的特征向量的长度来进行归一化。
>	

归一化和标准化的使用场景：
- 如果对输出结果范围有要求，则用归一化
- 如果数据较为稳定，不存在极端的最大值和最小值，则用归一化
- 如果数据存在异常值和较多噪声，则用标准化，这样可以通过中心化简介避免异常值和极端值的影响
- 支持向量机、K近邻、主成分分析等模型必须进行归一化或者标准化操作

# 三 特征降维
特征降维是指的采用某种映射方法，将高维向量空间的数据点映射到低维空间。整个数据集是一个矩阵，特征降维就是找到空间变换的变换矩阵，即f函数，将其从高维空间降到低维空间，f函数的定义比较灵活，可以使显式的或隐式的，也可以是线性的或非线性的。[[空间变换]]
在原始的高维空间中，向量数据包含冗余信息和噪音信息，会对模型产生误差，降低准确率，而通过特征降维可以减少这些信息造成的误差，从而提高识别的精度。

特征降维的方法有特征选择和线性降维。

## 3.1 特征选择
特征选择是在数据分析和简单建模中最常用的特征降维手段。
手段比较粗暴，即映射函数直接将不重要的特征删除，不过这样会造成特征信息丢失，不利于模型精度。
由于数据分析以专注主要影响因子为主，变量越少越有利于分析，因此特征选择常用于统计分析模型；在超高维数据分析或者建模预处理中也会经常使用。

### 3.1.1  特征选择的目的
- 特征是否发散：如果一个特征不发散，如方差接近于0，就说明样本在整个特征上基本没有差异，整个特征对与样本的区分没有作用。
- 特征与目标的相关性：与目标相关性高的特征应当优先选择

### 3.1.2 特征选择的方法

- 过滤法（filter）：按照发散性或者相关性对各个特征进行评分，通过设定阈值或者待选择阈值的个数来选择特征
	思路：特征变连个和目标变量之间的关系
	相关系数
	卡方检验
	信息增益、互信息
- 包装法（wrapper）：根据目标函数（通常是预测效果评分）每次选择若干特征，或者排除若干特征
	思路：通过目标函数（AUC/MSE）来决定是否加入一个变量
	迭代：产生特征子集
	- 评价;
		- 完全搜索
		- 启发搜索
		- 随机搜索
			- 遗传算法GA（Genetic Algorithm)
			- 模拟退货算法SA（Simulated Annealing)
- 嵌入法（embedded）：使用机器学习的某些算法和模型进行训练，得到各个特征的权值系数，并根据系数从大到小选择特征。这一方法类似于过滤法，区别在于它通过训练来确定特征的优劣
	思路：学习期自身自动选择特征
	正则化：
	- L1: LASSO
	- L2：RIDGE
	决策树：熵、信息增益 （信息增益的链接[[决策树]]）
	深度学习

### 3.1.3 特征选择实现

| 类                 | 所属方法     | 具体方法                         |
| ----------------- | -------- | ---------------------------- |
| VarianceThreshold | Filter   | 方差选择法                        |
| SelectBest        | Filter   | 将可选相关系数、卡方检验、最大信息系数作为得分计算的方法 |
| RFE               | Wrapper  | 递归消除特征法                      |
| SelectFromModel   | Embedded | 基于模型的特征选择法                   |
- VarianceThreshold
	使用方差选择法，先要计算各个特征的方差，然后根据阈值选择方差大于阈值的特征，使用feature_selection的VarianceThreshold类来选择特征。
- SelectBest
	- 相关系数法
		使用相关系数法，先计算各个特征对目标值的相关系数及相关系数P值，然后根据阈值筛选特征
	- 卡方检验
		经典的卡方检验是检验定性自变量与定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等i且因变量等于j的样本聘书的观察值与期望差距，构建统计量
$$
		x^2=\sum_{}^{}\frac{(A-E)^2}{E}
$$
	    
	    最大信息系数
- RFE
	递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练
- SelectFromModel
	SelectFromModel主要采用基于模型的特征选择法，常见的有基于惩罚项的特征选择法和基于树模型的特征选择法
	1. 基于惩罚项的特征选择法使用带惩罚项的基模型，除了能筛选出特征，也进行了降维
	2. 基于树模型的特征选择法，在树模型中，GBDT [[GBDT 梯度提升决策树]] 也可以用来作为基模型进行特征选择

## 3.2 线性降维

### 3.2.1 主成分分析法（PCA)
常用的方法有主成分分析和线性判别分析法
主成分分析法是最常用的线性降维方法，主要原理使用过某种线性投影，将高维的数据映射到低维的空间中表示，并期望在所投影的维度上数据的方差最大，以此达到使用较少的数据维度来保留较多的原数据点特性的效果。

### 3.2.2 线性判别分析法（LDA）
线性判别分析法，也叫做Fisher线性判别（Fisher Linear Discriminant， FLD），是一种有监督的线性降维算法，与PCA尽可能多地保留数据信息不同，LDA的目标是使降维后的数据点尽可能地容易被区分，其利用了标签的信息。
![image.png](https://build-web.oss-cn-qingdao.aliyuncs.com/my_pic_file/20240803151824.png)
