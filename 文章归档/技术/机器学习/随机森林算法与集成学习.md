#数据科学  #机器学习 

[[决策树]] 
# 1 集成学习

![image.png](https://build-web.oss-cn-qingdao.aliyuncs.com/my_pic_file/20250301082338.png)


![image.png](https://build-web.oss-cn-qingdao.aliyuncs.com/my_pic_file/20250301082348.png)

分类任务用左边式子，少数服从多数的原则
回归任务用右边式子，求平均


![image.png](https://build-web.oss-cn-qingdao.aliyuncs.com/my_pic_file/20250301082355.png)


集成学习是多个不同的学习器通过投票或者给予不同的权重组合来实现

如何生成不一样的g（x）

- 同样的数据，行列都相同，不同的超参数，可以得到不同的模型
- 同样的超参数，行相同，列不同，可以得到不同的模型
- 同样的超参数，列相同，行不同，可以得到不同的模型
- 同样的超参数，同样的数据，但是数据权重不同，可以得到不同的模型

## 1.1 Bagging

先理解下什么是bagging？

![image.png](https://build-web.oss-cn-qingdao.aliyuncs.com/my_pic_file/20250301082403.png)


Bagging算法（Bootstrap aggregating，引导聚集算法），又称装袋算法，是机器学习领域的一种集成学习算法

Bagging是通过结合几个模型降低泛化误差的技术。主要想法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出。这是机器学习中常规策略的一个例子，被称为模型平均。采用这种策略的技术被称为集成方法。举个例子，给定一个大小为n的[训练集](https://baike.baidu.com/item/%E8%AE%AD%E7%BB%83%E9%9B%86?fromModule=lemma_inlink)D，Bagging算法从中均匀、有放回地（即使用自助抽样法）选出m个大小为n'的[子集](https://baike.baidu.com/item/%E5%AD%90%E9%9B%86?fromModule=lemma_inlink)Di，作为新的训练集。在这m个训练集上使用分类、回归等算法，则可得到m个模型，再通过取[平均值](https://baike.baidu.com/item/%E5%B9%B3%E5%9D%87%E5%80%BC?fromModule=lemma_inlink)、取多数票等方法，即可得到Bagging的结果 

Bagging的特性：

1. Bagging通过降低基分类器的方差，改善了泛化误差
2. 其性能依赖于基分类器的稳定性；如果基分类器不稳定，bagging有助于降低训练数据的随机波动导致的误差；如果稳定，则集成分类器的误差主要由基分类器的偏倚引起
3. 由于每个样本被选中的概率相同，因此bagging并不侧重于训练数据集中的任何特定实例

Bootstrap : 有放回地对原始数据集进行均匀抽样
利用每次抽样生成的数据集训练模型
最终的模型为每次生成的模型进行投票
其实boosting和bagging都不仅局限于对决策树这种基模型适应
如果不是同一种base model，stacking

## 1.2 Boosting

利用训练集训练出的模型根据本次模型的预测结果，调整训练集，然后利用调整后的训练集训练下一个模型。串行，需要第一个模型。adaboost，GBDT,  Xgboost

1. 权重不一样，加权求和
2. 串行，根据第一棵树的结果来训练第二棵树，依次类推下去

# 2 随机森林

Bagging思想+决策树作为 base model+uniform blending

![image.png](https://build-web.oss-cn-qingdao.aliyuncs.com/my_pic_file/20250301082413.png)


![image.png](https://build-web.oss-cn-qingdao.aliyuncs.com/my_pic_file/20250301082422.png)


随机随机在哪？

1. bagging生成小树时随机抽取数据
2. 抽取数据后，对feature也进行抽样

## 随机森林表现

![image.png](https://build-web.oss-cn-qingdao.aliyuncs.com/my_pic_file/20250301082432.png)


![image.png](https://build-web.oss-cn-qingdao.aliyuncs.com/my_pic_file/20250301082442.png)


## 总结

- 随机森林
    1. 随机选择样本（有放回抽样）
    2. 随机选择特征
    3. 构建决策树
    4. 随机森林投票（平均）
- 优点
    1. 表现良好
    2. 可以处理高维度数据（维度随机选择）
    3. 辅助进行特征选择
    4. 得益于bagging可进行并行训练
- 缺点
    
    对于噪声过大的数据容易过拟合