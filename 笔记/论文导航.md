# ğŸ“œ å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®ºæ–‡å¯¼èˆª

ä¸€ä»½ç²¾å¿ƒæ•´ç†çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç›¸å…³è®ºæ–‡åˆ—è¡¨ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å¿«é€Ÿæ‰¾åˆ°é‡è¦æ–‡çŒ®ã€‚

## ğŸ“š ç›®å½•

- [ğŸ“œ å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®ºæ–‡å¯¼èˆª](#-å¤§è¯­è¨€æ¨¡å‹llmè®ºæ–‡å¯¼èˆª)
  - [ğŸ“š ç›®å½•](#-ç›®å½•)
  - [ğŸ¤– LLM åŸç†](#-llm-åŸç†)
  - [ğŸ› ï¸ å¾®è°ƒ](#ï¸-å¾®è°ƒ)
  - [ğŸ” RAG](#-rag)
  - [ğŸ§  Agent](#-agent)
  - [ğŸ’¡ åº”ç”¨](#-åº”ç”¨)
  - [ğŸ“Š è¯„ä¼°](#-è¯„ä¼°)

---

## ğŸ¤– LLM åŸç†

| è®ºæ–‡æ ‡é¢˜ | ä½œè€… | å‘è¡¨å¹´ä»½ | é“¾æ¥ | ç®€ä»‹ |
| :--- | :--- | :--- | :--- | :--- |
| Attention Is All You Need | Ashish Vaswani, et al. | 2017 | [Link](https://arxiv.org/abs/1706.03762) | æå‡ºäº† Transformer æ¨¡å‹ï¼Œå½»åº•æ”¹å˜äº† NLP é¢†åŸŸã€‚ |
| BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | Jacob Devlin, et al. | 2018 | [Link](https://arxiv.org/abs/1810.04805) | æå‡ºäº† BERT æ¨¡å‹ï¼Œé€šè¿‡åŒå‘ Transformer é¢„è®­ç»ƒå®ç° SOTAã€‚ |
| Language Models are Few-Shot Learners | Tom B. Brown, et al. | 2020 | [Link](https://arxiv.org/abs/2005.14165) | ä»‹ç»äº† GPT-3ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚ |

---

## ğŸ› ï¸ å¾®è°ƒ

| è®ºæ–‡æ ‡é¢˜ | ä½œè€… | å‘è¡¨å¹´ä»½ | é“¾æ¥ | ç®€ä»‹ |
| :--- | :--- | :--- | :--- | :--- |
| Parameter-Efficient Transfer Learning for NLP | Neil Houlsby, et al. | 2019 | [Link](https://arxiv.org/abs/1902.00751) | æå‡ºäº† Adapter-tuningï¼Œä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ã€‚ |
| LoRA: Low-Rank Adaptation of Large Language Models | Edward J. Hu, et al. | 2021 | [Link](https://arxiv.org/abs/2106.09685) | æå‡ºäº† LoRAï¼Œé€šè¿‡ä½ç§©åˆ†è§£æ¥é«˜æ•ˆå¾®è°ƒå¤§æ¨¡å‹ã€‚ |
| QLoRA: Efficient Finetuning of Quantized LLMs | Tim Dettmers, et al. | 2023 | [Link](https://arxiv.org/abs/2305.14314) | ç»“åˆé‡åŒ–å’Œ LoRAï¼Œå®ç°äº†åœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šå¾®è°ƒå¤§æ¨¡å‹ã€‚ |

---

## ğŸ” RAG

| è®ºæ–‡æ ‡é¢˜ | ä½œè€… | å‘è¡¨å¹´ä»½ | é“¾æ¥ | ç®€ä»‹ |
| :--- | :--- | :--- | :--- | :--- |
| Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks | Patrick Lewis, et al. | 2020 | [Link](https://arxiv.org/abs/2005.11401) | é¦–æ¬¡æå‡º RAG æ¡†æ¶ï¼Œç»“åˆäº†é¢„è®­ç»ƒæ¨¡å‹å’Œéå‚æ•°è®°å¿†ã€‚ |
| Self-Correction for Long-Term Factuality in Large Language Models | Howard Chen, et al. | 2023 | [Link](https://arxiv.org/abs/2305.14962) | æ¢ç´¢äº†å¦‚ä½•é€šè¿‡è‡ªæˆ‘ä¿®æ­£æ¥æå‡ RAG ç³»ç»Ÿçš„é•¿æœŸäº‹å®å‡†ç¡®æ€§ã€‚ |
| Corrective Retrieval Augmented Generation | Zichao Li, et al. | 2024 | [Link](https://arxiv.org/abs/2401.15884) | æå‡ºäº†ä¸€ç§çº æ­£æ€§æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ï¼Œä»¥æé«˜ç”Ÿæˆè´¨é‡ã€‚ |

---

## ğŸ§  Agent

| è®ºæ–‡æ ‡é¢˜ | ä½œè€… | å‘è¡¨å¹´ä»½ | é“¾æ¥ | ç®€ä»‹ |
| :--- | :--- | :--- | :--- | :--- |
| ReAct: Synergizing Reasoning and Acting in Language Models | Shunyu Yao, et al. | 2022 | [Link](https://arxiv.org/abs/2210.03629) | æå‡ºäº† ReAct æ¡†æ¶ï¼Œä½¿ LLM èƒ½å¤Ÿç»“åˆæ¨ç†å’Œè¡ŒåŠ¨ã€‚ |
| Toolformer: Language Models That Teach Themselves to Use Tools | Timo Schick, et al. | 2023 | [Link](https://arxiv.org/abs/2302.04761) | è®­ç»ƒ LLM è‡ªä¸»å­¦ä¹ ä½¿ç”¨å¤–éƒ¨å·¥å…·ï¼ˆAPIï¼‰ã€‚ |
| Generative Agents: Interactive Simulacra of Human Behavior | Joon Sung Park, et al. | 2023 | [Link](https://arxiv.org/abs/2304.03442) | åˆ›å»ºäº†èƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»è¡Œä¸ºçš„ç”Ÿæˆå¼æ™ºèƒ½ä½“ã€‚ |

---

## ğŸ’¡ åº”ç”¨

| è®ºæ–‡æ ‡é¢˜ | ä½œè€… | å‘è¡¨å¹´ä»½ | é“¾æ¥ | ç®€ä»‹ |
| :--- | :--- | :--- | :--- | :--- |
| CodeX: Evaluating Large Language Models on Code | Mark Chen, et al. | 2021 | [Link](https://arxiv.org/abs/2107.03374) | ä»‹ç»äº† CodeXï¼Œä¸€ä¸ªåœ¨ä»£ç ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²çš„ LLMã€‚ |
| Chain-of-Thought Prompting Elicits Reasoning in Large Language Models | Jason Wei, et al. | 2022 | [Link](https://arxiv.org/abs/2201.11903) | æå‡ºäº†æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºï¼Œä»¥æ¿€å‘ LLM çš„æ¨ç†èƒ½åŠ›ã€‚ |
| Large Language Models as Optimizers | Chengrun Yang, et al. | 2023 | [Link](https://arxiv.org/abs/2309.03409) | æ¢ç´¢äº†ä½¿ç”¨ LLM ä½œä¸ºä¼˜åŒ–å™¨çš„æ½œåŠ›ã€‚ |

---

## ğŸ“Š è¯„ä¼°

| è®ºæ–‡æ ‡é¢˜ | ä½œè€… | å‘è¡¨å¹´ä»½ | é“¾æ¥ | ç®€ä»‹ |
| :--- | :--- | :--- | :--- | :--- |
| GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding | Alex Wang, et al. | 2018 | [Link](https://arxiv.org/abs/1804.07461) | æå‡ºäº† GLUEï¼Œä¸€ä¸ªå¹¿æ³›ä½¿ç”¨çš„ NLU è¯„ä¼°åŸºå‡†ã€‚ |
| SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems | Alex Wang, et al. | 2019 | [Link](https://arxiv.org/abs/1905.00537) | GLUE çš„å‡çº§ç‰ˆï¼ŒåŒ…å«æ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ |
| MMLU: Measuring Massive Multitask Language Understanding | Dan Hendrycks, et al. | 2020 | [Link](https://arxiv.org/abs/2009.03300) | æå‡ºäº† MMLUï¼Œä¸€ä¸ªç”¨äºè¯„ä¼° LLM å¤šä»»åŠ¡çŸ¥è¯†çš„åŸºå‡†ã€‚ |
